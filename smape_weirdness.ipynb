{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from feature_engine.outliers import Winsorizer\n",
    "from sklearn.dummy import DummyRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler,MinMaxScaler\n",
    "from feature_engine.outliers import Winsorizer\n",
    "from sklearn.compose import TransformedTargetRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn import set_config, get_config\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer\n",
    "from pprint import pprint\n",
    "from collections import defaultdict\n",
    "import tools\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "set_config(transform_output=\"pandas\")\n",
    "from mlxtend.evaluate.time_series import GroupTimeSeriesSplit, plot_splits, print_cv_info, print_split_info\n",
    "\n",
    "\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.options.display.float_format = '{:.2f}'.format\n",
    "import warnings; warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_train = pd.read_csv('./data/raw/godaddy-microbusiness-density-forecasting/train.csv')\n",
    "new_train = pd.read_csv('./data/raw/godaddy-microbusiness-density-forecasting_new/revealed_test.csv')\n",
    "\n",
    "old_test = pd.read_csv('./data/raw/godaddy-microbusiness-density-forecasting/test.csv')\n",
    "sample_submission = pd.read_csv('./data/raw/godaddy-microbusiness-density-forecasting/sample_submission.csv')\n",
    "\n",
    "train = pd.concat((old_train, new_train))\n",
    "test = old_test[~old_test['first_day_of_month'].isin(new_train['first_day_of_month'])]\n",
    "\n",
    "train['is_test'] = 0 ; test['is_test'] = 1\n",
    "\n",
    "data = pd.concat((\n",
    "        train,\n",
    "        test)\n",
    "        )\\\n",
    "    .reset_index(drop=True)\\\n",
    "    .assign(\n",
    "        cfips = lambda df: df['cfips'].astype(str).str.zfill(5),\n",
    "        date = lambda df: pd.to_datetime(df[\"first_day_of_month\"]),\n",
    "        mdensity_t0 = lambda df: df['microbusiness_density'],\n",
    "        active_t0 = lambda df: df['active'],\n",
    "        )\\\n",
    "    .sort_values(['cfips','date'], ascending=True)\\\n",
    "    .assign(\n",
    "    \n",
    "        state_i = lambda df: df['cfips'].apply(lambda x: x[:2]),\n",
    "        county_i = lambda df: df['cfips'].apply(lambda x: x[2:]),\n",
    "        \n",
    "        year = lambda df: df['date'].dt.year,\n",
    "        date = lambda df: df[\"date\"].dt.date,\n",
    "        # month = lambda df: df['date'].dt.month,\n",
    "\n",
    "        dcount = lambda df: df.groupby('cfips')['row_id'].cumcount(),\n",
    "        \n",
    "        active_lag1 = lambda df: df.groupby('cfips')['active_t0'].shift(1),\n",
    "        active_lag2 = lambda df: df.groupby('cfips')['active_t0'].shift(2),\n",
    "        active_lag3 = lambda df: df.groupby('cfips')['active_t0'].shift(3),\n",
    "        \n",
    "        target_0 = lambda df: np.nan_to_num(df['active']),\n",
    "        target_1 = lambda df: np.nan_to_num(df['active']),\n",
    "        target_2 = lambda df: np.nan_to_num(df['active']),\n",
    "\n",
    "    \n",
    "\n",
    "    )\\\n",
    "    .drop(['county','state'], axis='columns')\n",
    "# .sort_index(ascending=True)\n",
    "\n",
    "assert all(data.groupby('cfips')['county_i'].nunique() == 1)\n",
    "assert all(data.groupby('cfips')['state_i'].nunique() == 1)\n",
    "assert data['cfips'].nunique() == 3135 # there are 3135 county,state tuples\n",
    "assert data['dcount'].nunique() == 47 # there are 47 series for each county state tuple\n",
    "assert data.query('is_test==0')['dcount'].nunique() == 41 # there are 41 series in the train set. \n",
    "assert data.query('is_test==1')['dcount'].nunique() == 6  # there are 6 series in the test set. \n",
    "\n",
    "#The private leaderboard will include 03-2023, 04-2023, 05-2023\n",
    "#The public leaderboard includes the first month 11-2022. Probably it will be updated later as 12-2022,01-2023 and 02-2023\n",
    "#The LB is updated as 01-2023\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding census data\n",
    "data_census = []\n",
    "for year in range(2017,2022):\n",
    "    COLS = ['GEO_ID','NAME','S0101_C01_026E']\n",
    "    data_census_i = pd.read_csv(f'./data/raw/census_data_1/ACSST5Y{year}.S0101-Data.csv',usecols=COLS)\n",
    "    data_census_i = data_census_i.iloc[1:]\n",
    "    data_census_i['population'] = data_census_i['S0101_C01_026E'].astype('int')\n",
    "\n",
    "\n",
    "    data_census_i['cfips'] = data_census_i.GEO_ID.apply(lambda x: f\"{int(x.split('US')[-1]):05}\" )\n",
    "    data_census_i['year'] = year+2\n",
    "    data_census.append(data_census_i[['cfips','year','population']])\n",
    "\n",
    "data_census = pd.concat((data_census),axis='rows')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.merge(data_census, on=['cfips','year'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_id</th>\n",
       "      <th>cfips</th>\n",
       "      <th>first_day_of_month</th>\n",
       "      <th>microbusiness_density</th>\n",
       "      <th>active</th>\n",
       "      <th>is_test</th>\n",
       "      <th>date</th>\n",
       "      <th>mdensity_t0</th>\n",
       "      <th>active_t0</th>\n",
       "      <th>state_i</th>\n",
       "      <th>county_i</th>\n",
       "      <th>year</th>\n",
       "      <th>dcount</th>\n",
       "      <th>active_lag1</th>\n",
       "      <th>active_lag2</th>\n",
       "      <th>active_lag3</th>\n",
       "      <th>target_0</th>\n",
       "      <th>target_1</th>\n",
       "      <th>target_2</th>\n",
       "      <th>population</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1001_2019-08-01</td>\n",
       "      <td>01001</td>\n",
       "      <td>2019-08-01</td>\n",
       "      <td>3.01</td>\n",
       "      <td>1249.00</td>\n",
       "      <td>0</td>\n",
       "      <td>2019-08-01</td>\n",
       "      <td>3.01</td>\n",
       "      <td>1249.00</td>\n",
       "      <td>01</td>\n",
       "      <td>001</td>\n",
       "      <td>2019</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1249.00</td>\n",
       "      <td>1249.00</td>\n",
       "      <td>1249.00</td>\n",
       "      <td>41527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1001_2019-09-01</td>\n",
       "      <td>01001</td>\n",
       "      <td>2019-09-01</td>\n",
       "      <td>2.88</td>\n",
       "      <td>1198.00</td>\n",
       "      <td>0</td>\n",
       "      <td>2019-09-01</td>\n",
       "      <td>2.88</td>\n",
       "      <td>1198.00</td>\n",
       "      <td>01</td>\n",
       "      <td>001</td>\n",
       "      <td>2019</td>\n",
       "      <td>1</td>\n",
       "      <td>1249.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1198.00</td>\n",
       "      <td>1198.00</td>\n",
       "      <td>1198.00</td>\n",
       "      <td>41527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1001_2019-10-01</td>\n",
       "      <td>01001</td>\n",
       "      <td>2019-10-01</td>\n",
       "      <td>3.06</td>\n",
       "      <td>1269.00</td>\n",
       "      <td>0</td>\n",
       "      <td>2019-10-01</td>\n",
       "      <td>3.06</td>\n",
       "      <td>1269.00</td>\n",
       "      <td>01</td>\n",
       "      <td>001</td>\n",
       "      <td>2019</td>\n",
       "      <td>2</td>\n",
       "      <td>1198.00</td>\n",
       "      <td>1249.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1269.00</td>\n",
       "      <td>1269.00</td>\n",
       "      <td>1269.00</td>\n",
       "      <td>41527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1001_2019-11-01</td>\n",
       "      <td>01001</td>\n",
       "      <td>2019-11-01</td>\n",
       "      <td>2.99</td>\n",
       "      <td>1243.00</td>\n",
       "      <td>0</td>\n",
       "      <td>2019-11-01</td>\n",
       "      <td>2.99</td>\n",
       "      <td>1243.00</td>\n",
       "      <td>01</td>\n",
       "      <td>001</td>\n",
       "      <td>2019</td>\n",
       "      <td>3</td>\n",
       "      <td>1269.00</td>\n",
       "      <td>1198.00</td>\n",
       "      <td>1249.00</td>\n",
       "      <td>1243.00</td>\n",
       "      <td>1243.00</td>\n",
       "      <td>1243.00</td>\n",
       "      <td>41527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1001_2019-12-01</td>\n",
       "      <td>01001</td>\n",
       "      <td>2019-12-01</td>\n",
       "      <td>2.99</td>\n",
       "      <td>1243.00</td>\n",
       "      <td>0</td>\n",
       "      <td>2019-12-01</td>\n",
       "      <td>2.99</td>\n",
       "      <td>1243.00</td>\n",
       "      <td>01</td>\n",
       "      <td>001</td>\n",
       "      <td>2019</td>\n",
       "      <td>4</td>\n",
       "      <td>1243.00</td>\n",
       "      <td>1269.00</td>\n",
       "      <td>1198.00</td>\n",
       "      <td>1243.00</td>\n",
       "      <td>1243.00</td>\n",
       "      <td>1243.00</td>\n",
       "      <td>41527</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            row_id  cfips first_day_of_month  microbusiness_density  active  is_test        date  mdensity_t0  active_t0 state_i county_i  year  dcount  active_lag1  active_lag2  active_lag3  target_0  target_1  target_2  population\n",
       "0  1001_2019-08-01  01001         2019-08-01                   3.01 1249.00        0  2019-08-01         3.01    1249.00      01      001  2019       0          NaN          NaN          NaN   1249.00   1249.00   1249.00       41527\n",
       "1  1001_2019-09-01  01001         2019-09-01                   2.88 1198.00        0  2019-09-01         2.88    1198.00      01      001  2019       1      1249.00          NaN          NaN   1198.00   1198.00   1198.00       41527\n",
       "2  1001_2019-10-01  01001         2019-10-01                   3.06 1269.00        0  2019-10-01         3.06    1269.00      01      001  2019       2      1198.00      1249.00          NaN   1269.00   1269.00   1269.00       41527\n",
       "3  1001_2019-11-01  01001         2019-11-01                   2.99 1243.00        0  2019-11-01         2.99    1243.00      01      001  2019       3      1269.00      1198.00      1249.00   1243.00   1243.00   1243.00       41527\n",
       "4  1001_2019-12-01  01001         2019-12-01                   2.99 1243.00        0  2019-12-01         2.99    1243.00      01      001  2019       4      1243.00      1269.00      1198.00   1243.00   1243.00   1243.00       41527"
      ]
     },
     "execution_count": 312,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PARAMETERS\n",
    "n_SPLITS = 5 \n",
    "n_TRAIN_TRAIN_SIZE = 30\n",
    "n_TRAIN_PERIOD = n_TRAIN_TRAIN_SIZE + 3 + n_SPLITS - 1 \n",
    "\n",
    "\n",
    "TEST_DATES = list(np.sort(data.query('is_test==1')['date'].unique())[:3])\n",
    "TEST_PERIOD = list(np.sort(data.query('is_test==1')['dcount'].unique())[:3])\n",
    "\n",
    "TRAIN_PERIOD = list(np.sort(data.query('is_test==0')['dcount'].unique())[-n_TRAIN_PERIOD:])\n",
    "TRAIN_DATES = list(np.sort(data.query('is_test==0')['date'].unique())[-n_TRAIN_PERIOD:])\n",
    "\n",
    "LEAKAGE = ['mdensity_t0','active']\n",
    "TARGETS = ['target_0', 'target_1', 'target_2']\n",
    "FEATURES = ['population']\n",
    "LAG_TARGET = ['active_lag1', 'active_lag2', 'active_lag3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data[data['dcount'].isin(TEST_PERIOD)].head()\n",
    "# sample = data[data.cfips.isin(['01001'])] # sample = data[data.cfips.isin(['01001','56045'])]\n",
    "sample = data.copy()\n",
    "sample.loc[sample.is_test==1,TARGETS]  = np.nan\n",
    "sample = sample.set_index(['date','cfips']).sort_index().loc[TRAIN_DATES+TEST_DATES]\n",
    "sample = sample[['dcount','county_i'] + LAG_TARGET + TARGETS + FEATURES+ LEAKAGE]\n",
    "sample_train= sample.query(\"dcount in @TRAIN_PERIOD\") ; sample_test= sample.query(\"dcount in @TEST_PERIOD\")\n",
    "train_X = sample_train.drop(TARGETS,axis='columns') ; train_y = sample_train[TARGETS]\n",
    "test_X = sample_test.drop(TARGETS,axis='columns') ; test_y = sample_test[TARGETS]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipelining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic_pipelines = {}\n",
    "y_test_preds  = [] \n",
    "\n",
    "y_val_preds = defaultdict(list)\n",
    "errors = defaultdict(list)\n",
    "\n",
    "lag=1\n",
    "list_cols_model = [[f'active_lag{lag_i+model_i+1}' for lag_i in range(lag)] for model_i in range(3)]\n",
    "\n",
    "for model_i in range(3):\n",
    "\n",
    "    train_y_i = train_y.iloc[:, model_i]\n",
    "    \n",
    "    cv_args = {\"test_size\": 1, \"n_splits\": n_SPLITS, \"train_size\": n_TRAIN_TRAIN_SIZE, 'gap_size': 0}\n",
    "    cv = GroupTimeSeriesSplit(**cv_args)\n",
    "\n",
    "    # new_features = Pipeline([('select', SimpleFeatureEngineering(features=list_cols_model[model_i]))])\n",
    "    # print(list_cols_model[model_i])\n",
    "    raw_features = Pipeline([('select', tools.ColumnSelector(features=list_cols_model[model_i]))])\n",
    "    \n",
    "    merge_features_numeric = FeatureUnion([\n",
    "        # ('new_features', new_features),\n",
    "        ('raw_features', raw_features)\n",
    "    ])\n",
    "\n",
    "    final_features_numeric = Pipeline([\n",
    "                            ('merge_features',merge_features_numeric),\n",
    "                            # ('remove_outliers', Winsorizer(capping_method='iqr', tail='both',fold=3)),\n",
    "                            # ('standart_scaler', StandardScaler())\n",
    "                            ]\n",
    "                            )\n",
    "\n",
    "    \n",
    "    # model = TransformedTargetRegressor(regressor=DummyRegressor(strategy='median'), transformer=None)\n",
    "    model = TransformedTargetRegressor(regressor=tools.LagModel(), transformer=None)\n",
    "    \n",
    "    model_pipeline = Pipeline([\n",
    "        (\"transform\", final_features_numeric),\n",
    "        (\"model\", model)\n",
    "    ])\n",
    "\n",
    "    dic_pipelines[f'pipeline_model_{model_i}'] = model_pipeline\n",
    "    \n",
    "    # param_grid = {'model__regressor__strategy':['mean','median' ]}\n",
    "    param_grid = {}\n",
    "    grid = GridSearchCV(dic_pipelines[f'pipeline_model_{model_i}'], scoring=make_scorer(tools.SMAPE_1, greater_is_better=False), param_grid=param_grid, cv=cv)\n",
    "    grid.fit(train_X, train_y_i, groups=train_X['dcount'])\n",
    "\n",
    "    # print(grid.cv_results_)\n",
    "    \n",
    "    # print(grid.best_estimator_)\n",
    "    \n",
    "    ## CHECK\n",
    "    check_train_period = TRAIN_DATES[-1-n_TRAIN_TRAIN_SIZE: -1] \n",
    "    validation_period = TRAIN_DATES[-1] \n",
    "\n",
    "    best_model = grid.best_estimator_\n",
    "\n",
    "    best_model.fit(train_X.loc[check_train_period], train_y_i.loc[check_train_period])   \n",
    "    \n",
    "    y_val_pred =  best_model.predict(train_X.loc[validation_period])   \n",
    "    y_val_preds[f'target_{model_i}'] = y_val_pred\n",
    "    y_val_i = train_y_i.loc[validation_period]    \n",
    "    errors[f'error_{model_i}'] = tools.SMAPE_1(y_true=y_val_i, y_pred=y_val_pred)\n",
    "\n",
    "#     # INFERENCE\n",
    "    final_train_period = TRAIN_DATES[-n_TRAIN_TRAIN_SIZE:] \n",
    "\n",
    "    best_model.fit(train_X.loc[final_train_period], train_y_i.loc[final_train_period])   \n",
    "\n",
    "    y_test_pred =  best_model.predict(test_X.loc[TEST_DATES[model_i]] )\n",
    "    y_test_preds.append(y_test_pred)\n",
    "\n",
    "test_X['active'] = np.concatenate((y_test_preds))\n",
    "\n",
    "# prepare data for error analysis\n",
    "val_X = train_X.loc[validation_period]\n",
    "y_val_preds =  pd.DataFrame(y_val_preds, index=val_X.index)\n",
    "val_X = pd.concat((val_X, y_val_preds), axis=1)\n",
    "\n",
    "test_X['microbusiness_density'] = 100*test_X['active']/test_X['population']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(list,\n",
       "            {'error_0': 1.8892067573580058,\n",
       "             'error_1': 2.4787835188289034,\n",
       "             'error_2': 2.9725079862458226})"
      ]
     },
     "execution_count": 317,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # Prepare submission file\n",
    "\n",
    "# date_submission = '0203'\n",
    "# local_score = round(errors['error_0'],2)\n",
    "# model_name = 'activity_lag_1'\n",
    "\n",
    "# submission = tools.create_submission(test_X,date_submission, model_name, local_score, sample_submission)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ERROR ANALYSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sample = train_X.loc[TRAIN_DATE[-1:]]\n",
    "train_sample.iloc[np.argsort(errors)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y_sample = train_y.loc[TRAIN_DATE[-1:]]\n",
    "SMAPE_1(train_y_sample.values,np.ones(train_y_sample.shape[0])*np.median(train_y_sample))\n",
    "SMAPE_1(train_y.loc['2022-10-01'].values,train_y.loc['2022-09-01'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 169.0067448718508, 152.1912078949058\n",
    "# 1.0730837144785978, 1.109003414526155\n",
    "# 145.82845110893408, 136.07246720848954\n",
    "# 1.7214972858534574, 1.7328342300564805\n",
    "# 130.85211125280742, 124.7100031585511\n",
    "# 2.348016937375499, 2.3395521962826726"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "black_list = []\n",
    "black_list.extend(train_X.sort_values(['mdensity_t0'])[:50].reset_index()['cfips'].unique())\n",
    "black_list.extend(train_X.sort_values(['mdensity_t0'])[-50:].reset_index()['cfips'].unique())\n",
    "keep = list(set(train_X.reset_index()['cfips'].unique()) - set(black_list))\n",
    "# train_X.loc[(slice(None),keep)]\n",
    "train_X_sample = train_X.loc[(slice(None),keep),:].reset_index().set_index(['date','cfips'])\n",
    "train_y_sample = train_y.loc[(slice(None),keep),:].reset_index().set_index(['date','cfips'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X_sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.linear_model import HuberRegressor, Lasso, Ridge\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "def smape(Y_predict, Y_test):\n",
    "    result = np.linalg.norm(Y_predict - Y_test, axis = 1)\n",
    "    result = np.abs(result)\n",
    "    denom = np.linalg.norm(Y_predict, axis = 1)\n",
    "    denom += np.linalg.norm(Y_test, axis = 1)\n",
    "    result /= denom\n",
    "    result *= 100 * 2\n",
    "    result = np.mean(result)\n",
    "    return result\n",
    "epsilon = 1e-6\n",
    "param_search = np.arange(10, 200, 20)\n",
    "\n",
    "scores = []\n",
    "for i in param_search:\n",
    "    print(i)\n",
    "\n",
    "    # definition of ztransformation.\n",
    "\n",
    "    def ztransform1(Y, param=i):\n",
    "        return 1 / (param + Y)\n",
    "\n",
    "    # inverse transformation, Y = inverseZ(Z)\n",
    "\n",
    "    def inverseZ1(Z, param=i):\n",
    "        return -param + 1 / Z\n",
    "    \n",
    "    \n",
    "    model = TransformedTargetRegressor(GradientBoostingRegressor(loss='squared_error', n_estimators=50,max_depth=10),func= ztransform1, inverse_func=inverseZ1)\n",
    "\n",
    "    model.fit( train_X.loc['2022-05-01':'2022-09-01',['mdensity_lag1','mdensity_lag2','mdensity_lag3']], train_y.loc['2022-05-01':'2022-09-01',['target_0']]) \n",
    "        \n",
    "    print(SMAPE_1(epsilon+model.predict(train_X.loc['2022-02-01':'2022-09-01',['mdensity_lag1','mdensity_lag2','mdensity_lag3']]),train_y.loc['2022-02-01':'2022-09-01',['target_0']].values))\n",
    "    print(SMAPE_1(epsilon+model.predict(train_X.loc['2022-10-01',['mdensity_lag1','mdensity_lag2','mdensity_lag3']]),train_y.loc['2022-10-01',['target_0']].values))\n",
    "    \n",
    "# 160\n",
    "# 1.3528178494715637\n",
    "# 1.4203927419328115\n",
    "# 190\n",
    "# 1.3527584337361627\n",
    "# 1.4161016190620148\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ad1d413a8513d632047e434ee4038ec414e54bcf3dba008e5fb6dedaf37ce15f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
