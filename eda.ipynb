{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from feature_engine.outliers import Winsorizer\n",
    "\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.options.display.float_format = '{:.3f}'.format\n",
    "import warnings; warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.DataFrame({'a':[np.nan,1,2,3,np.nan]}).ffill()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SMAPE_1 (y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Symmetric Mean Absolute Percentage Error (SMAPE)\n",
    "    \"\"\"\n",
    "    y_true = np.array(y_true)\n",
    "    denominator = (y_true + np.abs(y_pred)) / 200.0\n",
    "    diff = np.abs(y_true - y_pred) / denominator\n",
    "    diff[denominator == 0] = 0.0\n",
    "    return np.mean(diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('./data/train.csv')\n",
    "test = pd.read_csv('./data/test.csv')\n",
    "sample_submission = pd.read_csv('./data/sample_submission.csv')\n",
    "\n",
    "\n",
    "train['is_test'] = 0 ; test['is_test'] = 1\n",
    "\n",
    "data = pd.concat((\n",
    "        train,\n",
    "        test)\n",
    "        )\\\n",
    "    .reset_index(drop=True)\\\n",
    "    .assign(\n",
    "        cfips = lambda df: df['cfips'].astype(str).str.zfill(5),\n",
    "        date = lambda df: pd.to_datetime(df[\"first_day_of_month\"]),\n",
    "        m_density = lambda df: df['microbusiness_density'],\n",
    "        )\\\n",
    "    .sort_values(['cfips','date'], ascending=True)\\\n",
    "    .assign(\n",
    "        state_i = lambda df: df['cfips'].apply(lambda x: x[:2]),\n",
    "        county_i = lambda df: df['cfips'].apply(lambda x: x[2:]),\n",
    "        year = lambda df: df['date'].dt.year,\n",
    "        month = lambda df: df['date'].dt.month,\n",
    "        dcount = lambda df: df.groupby('cfips')['row_id'].cumcount(),\n",
    "        \n",
    "        m_density_lag_1 = lambda df: df.groupby('cfips')['m_density'].shift(1),\n",
    "        m_density_lag_2 = lambda df: df.groupby('cfips')['m_density'].shift(2),\n",
    "        m_density_lag_3 = lambda df: df.groupby('cfips')['m_density'].shift(3),\n",
    "\n",
    "        target_1 = lambda df: np.where( df['m_density']==0, 0, (df['m_density']/(df['m_density_lag_1'])).clip(0,99) - 1),\n",
    "        target_2 = lambda df: np.where( df['m_density']==0, 0, (df['m_density']/(df['m_density_lag_2'])).clip(0,99) - 1),\n",
    "        target_3 = lambda df: np.where( df['m_density']==0, 0, (df['m_density']/(df['m_density_lag_3'])).clip(0,99) - 1),\n",
    "\n",
    "        )\\\n",
    "    [['cfips','date','dcount','county_i','state_i','month','year','is_test','active','m_density',\n",
    "        'm_density_lag_1','m_density_lag_2','m_density_lag_3','target_1','target_2', 'target_3'\n",
    "        ]]\n",
    "    # .sort_index(ascending=True)\n",
    "\n",
    "assert all(data.groupby('cfips')['county_i'].nunique() == 1)\n",
    "assert all(data.groupby('cfips')['state_i'].nunique() == 1)\n",
    "assert data['cfips'].nunique() == 3135 # there are 3135 county,state tuples\n",
    "assert data['dcount'].nunique() == 47 # there are 47 series for each county state tuple\n",
    "assert data.query('is_test==0')['dcount'].nunique() == 39 # there are 39 series in the train set. \n",
    "assert data.query('is_test==1')['dcount'].nunique() == 8  # there are 8 series in the test set. \n",
    "\n",
    "#The private leaderboard will include 03-2023, 04-2023, 05-2023\n",
    "#The public leaderboard includes the first month 11-2022. Probably it will be updated later as 12-2022,01-2023 and 02-2023\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.groupby(['cfips'])['target'].agg(lambda x: sum(x.isna())).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data['lag_minus1_density']  = data.groupby('cfips')['m_density'].shift(-1)\n",
    "# data\n",
    "# data.sort_values('target',ascending=False).head()\n",
    "# data['target'].quantile(.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data[data[\"cfips\"] == '17075']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['target_1'] = data['target_1'].fillna(0)\n",
    "data['target_2'] = data['target_2'].fillna(0)\n",
    "data['target_3'] = data['target_3'].fillna(0)\n",
    "\n",
    "capper = Winsorizer(capping_method='iqr',tail='both', fold=5)\n",
    "data['target_1'] = capper.fit_transform(data[['target_1']])\n",
    "data['target_2'] = capper.fit_transform(data[['target_2']])\n",
    "data['target_3'] = capper.fit_transform(data[['target_3']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # check\n",
    "# data['target_ratio'] = data['target_ratio'].abs()\n",
    "# data.groupby('dcount')['target_ratio'].sum().plot()\n",
    "\n",
    "# data['target_ratio_capped_1'] = data['target_ratio_capped_1'].abs()\n",
    "# data.groupby('dcount')['target_ratio_capped_1'].sum().plot()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # check\n",
    "# temp = data.groupby(['cfips']).agg({'target_ratio':['mean','median','std']})\n",
    "# temp.columns = ['mean','median','std']\n",
    "# temp['ratio'] = temp['std']/(temp['median']+1e-10)\n",
    "# temp = temp.sort_values('std',ascending=False)\n",
    "\n",
    "# LEVEL1,LEVEL2,LEVEL3 = 1,2,5\n",
    "# capper = Winsorizer(capping_method='iqr',tail='both', fold=LEVEL1)\n",
    "# data[f'target_ratio_clean_{LEVEL1}'] = capper.fit_transform(data[['target_ratio']])\n",
    "# capper = Winsorizer(capping_method='iqr',tail='both', fold=LEVEL2)\n",
    "# data[f'target_ratio_clean_{LEVEL2}'] = capper.fit_transform(data[['target_ratio']])\n",
    "# capper = Winsorizer(capping_method='iqr',tail='both', fold=LEVEL3)\n",
    "# data[f'target_ratio_clean_{LEVEL3}'] = capper.fit_transform(data[['target_ratio']])\n",
    "\n",
    "# for i in range(0,20):\n",
    "#     try:\n",
    "#         plt.figure()\n",
    "#         x = data[(data['cfips'] == temp.index[i]) & (data['is_test'] == 0)][['target_ratio',f'target_ratio_clean_{LEVEL1}',f'target_ratio_clean_{LEVEL2}',f'target_ratio_clean_{LEVEL3}']]\n",
    "#         # plt.plot(x[['target_ratio']].values.reshape(-1, 1))\n",
    "#         plt.plot(x[[f'target_ratio_clean_{LEVEL1}']].values.reshape(-1, 1),'--', label=f'{LEVEL1}')\n",
    "#         plt.plot(x[[f'target_ratio_clean_{LEVEL2}']].values.reshape(-1, 1),'--', label=f'{LEVEL2}')\n",
    "#         plt.plot(x[[f'target_ratio_clean_{LEVEL3}']].values.reshape(-1, 1),'--', label=f'{LEVEL3}')\n",
    "#         plt.legend()\n",
    "#     except Exception as e:\n",
    "#         print(e)\n",
    "#         print(i)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [],
   "source": [
    "# census_starter = pd.read_csv('./data/census_starter.csv')\n",
    "\n",
    "# census_starter = census_starter.assign(\n",
    "#     cfips = lambda x: x['cfips'].astype(str)\n",
    "#     )\\\n",
    "#     .set_index(['cfips']).sort_index(ascending=True)\n",
    "\n",
    "# colname_tuples = [('_'.join(e.split('_')[:-1]),e.split('_')[-1]) for e in census_starter.columns.tolist()]\n",
    "# new_index = pd.MultiIndex.from_tuples(colname_tuples, names=['category','year_info'])\n",
    "# census_starter = census_starter.set_axis(new_index, axis=1).stack(level=1)\n",
    "# census_starter = census_starter.reset_index()\n",
    "# census_starter['year_available'] = census_starter['year_info'].astype(int) + 2\n",
    "\n",
    "\n",
    "\n",
    "# census_starter = pd.read_csv('./data/census_starter.csv')\n",
    "\n",
    "# census_starter = census_starter.assign(\n",
    "#     cfips = lambda x: x['cfips'].astype(str)\n",
    "#     )\\\n",
    "#     .set_index(['cfips']).sort_index(ascending=True)\n",
    "\n",
    "# new_index = pd.MultiIndex.from_tuples([('_'.join(e.split('_')[:-1]),e.split('_')[-1]) for e in census_starter.columns.tolist()], names=['category','year'])\n",
    "# census_starter = census_starter.set_axis(new_index, axis=1).stack(level=1)\n",
    "# mean_census = census_starter.groupby(level='year').mean()\n",
    "# mean_census"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.dummy import DummyRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from mlxtend.evaluate.time_series import GroupTimeSeriesSplit, plot_splits, print_cv_info, print_split_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGETS = ['target_1', 'target_2', 'target_3']\n",
    "LAG_DENSITY = ['m_density_lag_1', 'm_density_lag_2', 'm_density_lag_3']\n",
    "TEST_DATE = ['2022-11-01','2022-12-01','2023-01-01']\n",
    "TEST_PERIOD = [39, 40, 41]\n",
    "\n",
    "# sample = data[data.cfips.isin(['01001'])]\n",
    "# sample = data[data.cfips.isin(['01001','56045'])]\n",
    "sample = data.copy()\n",
    "sample.loc[sample.is_test==1,TARGETS]  = np.nan\n",
    "\n",
    "sample = sample.set_index(['date','cfips'])\n",
    "sample  = sample.sort_index()['2022-01':'2023-01']\n",
    "sample = sample[['dcount', 'year','county_i','m_density','m_density_lag_1','m_density_lag_2','m_density_lag_3'] + TARGETS]\n",
    "\n",
    "\n",
    "sample_train= sample[sample['dcount']<39]\n",
    "sample_test= sample[sample['dcount']>=39]\n",
    "\n",
    "train_X = sample_train.drop(TARGETS,axis='columns')\n",
    "train_y = sample_train[TARGETS]\n",
    "\n",
    "test_X = sample_test.drop(TARGETS,axis='columns')\n",
    "test_y = sample_test[TARGETS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_score(X, y_true, y_pred,):\n",
    "    m_density_target = (y_true + 1) * X['m_density_lag_1']\n",
    "    m_density_pred = (y_pred + 1) * X['m_density_lag_1']\n",
    "    error = SMAPE_1(m_density_target, m_density_pred)\n",
    "    print('SMAPE SCORE',error)\n",
    "    return error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SMAPE SCORE 2.5315404042788443\n",
      "SMAPE SCORE 1.3239561547581042\n",
      "SMAPE SCORE 0.9259471304232395\n",
      "SMAPE SCORE 1.9234055010180007\n",
      "SMAPE SCORE 3.0996207404097373\n",
      "SMAPE SCORE 1.6980757479600894\n",
      "SMAPE SCORE 2.596288845324702\n",
      "SMAPE SCORE 2.4016135913874876\n",
      "SMAPE SCORE 2.8611749767166095\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from collections import defaultdict\n",
    "errors = defaultdict(list)\n",
    "\n",
    "test_preds = []\n",
    "for model_i in range(3):\n",
    "    cv_args = {\"test_size\": 1, \"n_splits\": 3, \"train_size\": 5, 'gap_size': model_i}\n",
    "    # cv_args = {\"test_size\": 1, \"n_splits\": 3, 'gap_size': model_i}\n",
    "    # plot_splits(sample, None, sample['dcount'], **cv_args)\n",
    "    # print_split_info(sample, None, sample['dcount'], **cv_args)\n",
    "    cv = GroupTimeSeriesSplit(**cv_args)\n",
    "\n",
    "    for fold_i, (train_index, val_index) in enumerate(cv.split(train_X, train_y, train_X['dcount'])):\n",
    "        # SPLIT DATA\n",
    "        \n",
    "        # print(\n",
    "        # # #'\\ntrain period:',np.unique(train_X.index[train_index].tolist()),\n",
    "        # '\\ntrain period:',np.unique(train_X['dcount'][train_index]).tolist(),\n",
    "        # # # '\\nvalidation_period:', np.unique(train_X.index[val_index].tolist()),\n",
    "        # '\\nvalidation_period:', np.unique(train_X['dcount'][val_index]).tolist(),\n",
    "        # # # '\\ntest_period:',TEST_DATE[model_i],\n",
    "        # # '\\ntest_period:',TEST_PERIOD[model_i],\n",
    "        # )\n",
    "        X_train, y_train = train_X.iloc[train_index], train_y.iloc[train_index, model_i]\n",
    "        X_val, y_val = train_X.iloc[val_index], train_y.iloc[val_index, model_i]\n",
    "        # display(X_train)\n",
    "        # display(y_val)\n",
    "        # display(test_X)\n",
    "        # MODEL\n",
    "        model = DummyRegressor(strategy=\"median\")\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_val)\n",
    "        # print(f\"for model {model_i+1} and fold {fold_i+1}\")\n",
    "        errors[model_i].append(check_score(X_val, y_val, y_pred))\n",
    "        \n",
    "    # INFERENCE\n",
    "    # model.fit(train_X.loc[['2022-10-01']], train_y.loc[['2022-10-01']].iloc[:, model_i])\n",
    "    model.fit(train_X, train_y.iloc[:, model_i])\n",
    "    \n",
    "    test_preds.append(\n",
    "         model.predict(test_X[test_X.dcount == TEST_PERIOD[model_i]])\n",
    "         )\n",
    "\n",
    "\n",
    "# target\n",
    "output = pd.DataFrame({'target':np.concatenate(test_preds)},index =test_X.index).reset_index()\n",
    "output = output.merge(train_X.groupby('cfips')['m_density'].last(), how='left', on='cfips')\n",
    "output = output.assign(\n",
    "    row_id = lambda df: df.apply(lambda df: \"{}_{}\".format(int(df['cfips']),df['date'].date()), axis='columns'),\n",
    "    microbusiness_density = lambda df: (df['target']+1) * df['m_density']\n",
    "    )[['row_id','microbusiness_density']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean error for model_0: 1.593814563153396\n",
      "std error for model_0: 0.6826908088336036\n",
      "\n",
      "mean error for model_1: 2.2403673297959426\n",
      "std error for model_1: 0.6145083086728033\n",
      "\n",
      "mean error for model_2: 2.6196924711429332\n",
      "std error for model_2: 0.18834359381634624\n",
      "\n",
      "overall error: 2.1512914546974233\n"
     ]
    }
   ],
   "source": [
    "all_errors= []\n",
    "for i,errors_i in errors.items():\n",
    "    print(f\"mean error for model_{i}:\",(np.mean(errors_i)))\n",
    "    print(f\"std error for model_{i}:\",(np.std(errors_i)))\n",
    "    print()\n",
    "    all_errors.append(errors_i)\n",
    "print(\"overall error:\" ,np.mean(all_errors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.concat((\n",
    "    output,\n",
    "    sample_submission[~sample_submission.row_id.isin(output.row_id)])\n",
    ")\n",
    "\n",
    "submission.to_csv(\"data/0126_mean_benchmark_submission.csv\",index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fistik",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9da8b7550d3f76431de586e2aa1c6351864e9705ff36bb0f6949bde979fcc422"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
