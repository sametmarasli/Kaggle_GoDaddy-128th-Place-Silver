{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from feature_engine.outliers import Winsorizer\n",
    "\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.options.display.float_format = '{:.3f}'.format\n",
    "import warnings; warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.DataFrame({'a':[np.nan,1,2,3,np.nan]}).ffill()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SMAPE_1 (y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Symmetric Mean Absolute Percentage Error (SMAPE)\n",
    "    \"\"\"\n",
    "    y_true = np.array(y_true)\n",
    "    denominator = (y_true + np.abs(y_pred)) / 200.0\n",
    "    diff = np.abs(y_true - y_pred) / denominator\n",
    "    diff[denominator == 0] = 0.0\n",
    "    return np.mean(diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('./data/train.csv')\n",
    "test = pd.read_csv('./data/test.csv')\n",
    "sample_submission = pd.read_csv('./data/sample_submission.csv')\n",
    "\n",
    "\n",
    "train['is_test'] = 0 ; test['is_test'] = 1\n",
    "\n",
    "data = pd.concat((\n",
    "        train,\n",
    "        test)\n",
    "        )\\\n",
    "    .reset_index(drop=True)\\\n",
    "    .assign(\n",
    "        cfips = lambda df: df['cfips'].astype(str).str.zfill(5),\n",
    "        date = lambda df: pd.to_datetime(df[\"first_day_of_month\"]),\n",
    "        mdensity_t0 = lambda df: df['microbusiness_density'],\n",
    "        )\\\n",
    "    .sort_values(['cfips','date'], ascending=True)\\\n",
    "    .assign(\n",
    "        state_i = lambda df: df['cfips'].apply(lambda x: x[:2]),\n",
    "        county_i = lambda df: df['cfips'].apply(lambda x: x[2:]),\n",
    "        year = lambda df: df['date'].dt.year,\n",
    "        month = lambda df: df['date'].dt.month,\n",
    "        dcount = lambda df: df.groupby('cfips')['row_id'].cumcount(),\n",
    "        \n",
    "        mdensity_lag1 = lambda df: df.groupby('cfips')['mdensity_t0'].shift(1),\n",
    "        mdensity_lag2 = lambda df: df.groupby('cfips')['mdensity_t0'].shift(2),\n",
    "        mdensity_lag3 = lambda df: df.groupby('cfips')['mdensity_t0'].shift(3),\n",
    "        \n",
    "        \n",
    "\n",
    "        target_1 = lambda df: np.where( df['mdensity_lag1']==0, 0, (df['mdensity_t0']/(df['mdensity_lag1'])).clip(0,99) - 1),\n",
    "        target_2 = lambda df: np.where( df['mdensity_lag2']==0, 0, (df['mdensity_t0']/(df['mdensity_lag2'])).clip(0,99) - 1),\n",
    "        target_3 = lambda df: np.where( df['mdensity_lag3']==0, 0, (df['mdensity_t0']/(df['mdensity_lag3'])).clip(0,99) - 1),\n",
    "        \n",
    "\n",
    "        )\\\n",
    "    [['cfips','date','dcount','county_i','state_i','month','year','is_test','active','mdensity_t0', \n",
    "    'mdensity_lag1','mdensity_lag2','mdensity_lag3',\n",
    "    'target_1','target_2', 'target_3'\n",
    "        ]]\n",
    "    # .sort_index(ascending=True)\n",
    "\n",
    "assert all(data.groupby('cfips')['county_i'].nunique() == 1)\n",
    "assert all(data.groupby('cfips')['state_i'].nunique() == 1)\n",
    "assert data['cfips'].nunique() == 3135 # there are 3135 county,state tuples\n",
    "assert data['dcount'].nunique() == 47 # there are 47 series for each county state tuple\n",
    "assert data.query('is_test==0')['dcount'].nunique() == 39 # there are 39 series in the train set. \n",
    "assert data.query('is_test==1')['dcount'].nunique() == 8  # there are 8 series in the test set. \n",
    "\n",
    "#The private leaderboard will include 03-2023, 04-2023, 05-2023\n",
    "#The public leaderboard includes the first month 11-2022. Probably it will be updated later as 12-2022,01-2023 and 02-2023\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['target_1'] = data['target_1'].fillna(0)\n",
    "data['target_2'] = data['target_2'].fillna(0)\n",
    "data['target_3'] = data['target_3'].fillna(0)\n",
    "\n",
    "capper = Winsorizer(capping_method='iqr',tail='both', fold=5)\n",
    "data['target_1'] = capper.fit_transform(data[['target_1']])\n",
    "data['target_2'] = capper.fit_transform(data[['target_2']])\n",
    "data['target_3'] = capper.fit_transform(data[['target_3']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # check\n",
    "# data['target_ratio'] = data['target_ratio'].abs()\n",
    "# data.groupby('dcount')['target_ratio'].sum().plot()\n",
    "\n",
    "# data['target_ratio_capped_1'] = data['target_ratio_capped_1'].abs()\n",
    "# data.groupby('dcount')['target_ratio_capped_1'].sum().plot()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # check\n",
    "# temp = data.groupby(['cfips']).agg({'target_ratio':['mean','median','std']})\n",
    "# temp.columns = ['mean','median','std']\n",
    "# temp['ratio'] = temp['std']/(temp['median']+1e-10)\n",
    "# temp = temp.sort_values('std',ascending=False)\n",
    "\n",
    "# LEVEL1,LEVEL2,LEVEL3 = 1,2,5\n",
    "# capper = Winsorizer(capping_method='iqr',tail='both', fold=LEVEL1)\n",
    "# data[f'target_ratio_clean_{LEVEL1}'] = capper.fit_transform(data[['target_ratio']])\n",
    "# capper = Winsorizer(capping_method='iqr',tail='both', fold=LEVEL2)\n",
    "# data[f'target_ratio_clean_{LEVEL2}'] = capper.fit_transform(data[['target_ratio']])\n",
    "# capper = Winsorizer(capping_method='iqr',tail='both', fold=LEVEL3)\n",
    "# data[f'target_ratio_clean_{LEVEL3}'] = capper.fit_transform(data[['target_ratio']])\n",
    "\n",
    "# for i in range(0,20):\n",
    "#     try:\n",
    "#         plt.figure()\n",
    "#         x = data[(data['cfips'] == temp.index[i]) & (data['is_test'] == 0)][['target_ratio',f'target_ratio_clean_{LEVEL1}',f'target_ratio_clean_{LEVEL2}',f'target_ratio_clean_{LEVEL3}']]\n",
    "#         # plt.plot(x[['target_ratio']].values.reshape(-1, 1))\n",
    "#         plt.plot(x[[f'target_ratio_clean_{LEVEL1}']].values.reshape(-1, 1),'--', label=f'{LEVEL1}')\n",
    "#         plt.plot(x[[f'target_ratio_clean_{LEVEL2}']].values.reshape(-1, 1),'--', label=f'{LEVEL2}')\n",
    "#         plt.plot(x[[f'target_ratio_clean_{LEVEL3}']].values.reshape(-1, 1),'--', label=f'{LEVEL3}')\n",
    "#         plt.legend()\n",
    "#     except Exception as e:\n",
    "#         print(e)\n",
    "#         print(i)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# census_starter = pd.read_csv('./data/census_starter.csv')\n",
    "\n",
    "# census_starter = census_starter.assign(\n",
    "#     cfips = lambda x: x['cfips'].astype(str)\n",
    "#     )\\\n",
    "#     .set_index(['cfips']).sort_index(ascending=True)\n",
    "\n",
    "# colname_tuples = [('_'.join(e.split('_')[:-1]),e.split('_')[-1]) for e in census_starter.columns.tolist()]\n",
    "# new_index = pd.MultiIndex.from_tuples(colname_tuples, names=['category','year_info'])\n",
    "# census_starter = census_starter.set_axis(new_index, axis=1).stack(level=1)\n",
    "# census_starter = census_starter.reset_index()\n",
    "# census_starter['year_available'] = census_starter['year_info'].astype(int) + 2\n",
    "\n",
    "\n",
    "\n",
    "# census_starter = pd.read_csv('./data/census_starter.csv')\n",
    "\n",
    "# census_starter = census_starter.assign(\n",
    "#     cfips = lambda x: x['cfips'].astype(str)\n",
    "#     )\\\n",
    "#     .set_index(['cfips']).sort_index(ascending=True)\n",
    "\n",
    "# new_index = pd.MultiIndex.from_tuples([('_'.join(e.split('_')[:-1]),e.split('_')[-1]) for e in census_starter.columns.tolist()], names=['category','year'])\n",
    "# census_starter = census_starter.set_axis(new_index, axis=1).stack(level=1)\n",
    "# mean_census = census_starter.groupby(level='year').mean()\n",
    "# mean_census"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.dummy import DummyRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from mlxtend.evaluate.time_series import GroupTimeSeriesSplit, plot_splits, print_cv_info, print_split_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.059</td>\n",
       "      <td>1.751</td>\n",
       "      <td>2.499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.153</td>\n",
       "      <td>0.199</td>\n",
       "      <td>0.395</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         0     1     2\n",
       "mean 1.059 1.751 2.499\n",
       "std  0.153 0.199 0.395"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "1.059"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TARGETS = ['target_1', 'target_2', 'target_3']\n",
    "LAG_DENSITY = ['mdensity_lag1', 'mdensity_lag2', 'mdensity_lag3',]\n",
    "\n",
    "TEST_DATE = ['2022-11-01','2022-12-01','2023-01-01']\n",
    "TEST_PERIOD = [39, 40, 41]\n",
    "\n",
    "LEAKAGE = ['mdensity_t0']\n",
    "\n",
    "TRAIN_SIZE = 3\n",
    "\n",
    "# sample = data[data.cfips.isin(['01001'])] # sample = data[data.cfips.isin(['01001','56045'])]\n",
    "sample = data.copy()\n",
    "sample.loc[sample.is_test==1,TARGETS]  = np.nan\n",
    "\n",
    "sample = sample.set_index(['date','cfips']).sort_index()['2022-01':'2023-01']\n",
    "sample = sample[['dcount', 'year','county_i'] + LAG_DENSITY + TARGETS + LEAKAGE]\n",
    "\n",
    "sample_train= sample[sample['dcount']< 39] ; sample_test= sample[sample['dcount']>=39]\n",
    "\n",
    "train_X = sample_train.drop(TARGETS,axis='columns') ; train_y = sample_train[TARGETS]\n",
    "test_X = sample_test.drop(TARGETS,axis='columns') ; test_y = sample_test[TARGETS]\n",
    "    \n",
    "\n",
    "def visualize_splits(model_no):\n",
    "        print(\n",
    "        \"model no\",model_no,\n",
    "        '\\ngs train period:',np.unique(train_X['dcount'][train_index]).tolist(),\n",
    "        '\\ngs validation_period:', np.unique(train_X['dcount'][val_index]).tolist(),\n",
    "        # '\\ninference train period:', list(range(train_X['dcount'].max()-TRAIN_SIZE+1, train_X['dcount'].max()+1)),\n",
    "        # '\\ninference test period:', [train_X['dcount'].max()+1+model_no]\n",
    "        )\n",
    "        \n",
    "from collections import defaultdict\n",
    "errors = defaultdict(list)\n",
    "test_preds = defaultdict(list)\n",
    "\n",
    "for model_i in range(3):\n",
    "    \n",
    "    cv_args = {\"test_size\": 1, \"n_splits\": 3, \"train_size\": TRAIN_SIZE, 'gap_size': model_i}\n",
    "    # plot_splits(sample, None, sample['dcount'], **cv_args)\n",
    "    # print_split_info(sample, None, sample['dcount'], **cv_args)\n",
    "    cv = GroupTimeSeriesSplit(**cv_args)\n",
    "    # y_val = []\n",
    "    for fold_i, (train_index, val_index) in enumerate(cv.split(train_X, train_y, train_X['dcount'])):\n",
    "        # SPLIT DATA\n",
    "        # visualize_splits(model_no = model_i)\n",
    "        X_train, y_train = train_X.iloc[train_index], train_y.iloc[train_index, model_i]\n",
    "        X_val, y_val = train_X.iloc[val_index], train_y.iloc[val_index, model_i]\n",
    "        \n",
    "        # MODEL\n",
    "        model = DummyRegressor(strategy=\"constant\", constant=0)\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_val)\n",
    "    \n",
    "        y_pred_target = (y_pred+1) * X_val[f'mdensity_lag{model_i+1}']\n",
    "        y_val_target = (y_val+1) * X_val[f'mdensity_lag{model_i+1}']\n",
    "\n",
    "        # print(SMAPE_1(y_true=y_val, y_pred=y_pred))\n",
    "        # print(SMAPE_1(y_true=y_val_target, y_pred=y_pred_target))\n",
    "        errors[f'{model_i}'].append(SMAPE_1(y_true=y_val_target, y_pred=y_pred_target))\n",
    "    # INFERENCE\n",
    "\n",
    "    model.fit(train_X, train_y.iloc[:, model_i])\n",
    "    # y_pred_test = model.predict(test_X[test_X.dcount == TEST_PERIOD[model_i]])\n",
    "    test_preds[f'target_{model_i}']= model.predict(test_X)\n",
    "\n",
    "    \n",
    "\n",
    "df_test_preds = pd.DataFrame(test_preds, index=test_X.index)\n",
    "df_test_preds = (df_test_preds + 1) * test_X[LAG_DENSITY].values\n",
    "\n",
    "# # check\n",
    "# df_output = pd.DataFrame(np.concatenate([df_test_preds.loc[TEST_DATE[i],f'target_{i}'] for i in range(3)]), index=test_X.index, columns = ['mdensity_t0'])\n",
    "# df_check = pd.concat((train_X[['mdensity_t0']], df_output), axis=0)\n",
    "# random_id = np.random.choice(df_check.index.levels[1])\n",
    "# df_check.loc[(slice(None),[random_id]),['mdensity_t0']].plot()\n",
    "# error_analysis\n",
    "df_errors = pd.DataFrame(errors)\n",
    "statistics_errors = df_errors.agg(['mean',np.std],axis=0)\n",
    "display(statistics_errors)\n",
    "local_score = statistics_errors.loc['mean'][0].round(3)\n",
    "local_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_output = pd.DataFrame(np.concatenate([df_test_preds.loc[TEST_DATE[i],f'target_{i}'] for i in range(3)]), index=test_X.index, columns = ['microbusiness_density'])\n",
    "# df_output = df_output.reset_index().assign(\n",
    "#     row_id = lambda df: df.apply(lambda df: \"{}_{}\".format(int(df['cfips']),df['date'].date()), axis='columns'))[['row_id','microbusiness_density']]\n",
    "\n",
    "# df_output\n",
    "\n",
    "# submission = pd.concat((\n",
    "#     df_output,\n",
    "#     sample_submission[~sample_submission.row_id.isin(df_output.row_id)])\n",
    "# )\n",
    "\n",
    "# submission.to_csv(f\"data/0203_median_local_{local_score}.csv\",index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.predict(X_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator\n",
    "\n",
    "class DummyModel(BaseEstimator):\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.zeros(X.shape[0])\n",
    "\n",
    "def check_score(X, y_true, y_pred):\n",
    "    m_density_target = (y_true + 1) * X[f'm_density_lag_1']\n",
    "    m_density_pred = (y_pred + 1) * X[f'm_density_lag_1']\n",
    "    error = SMAPE_1(m_density_target, m_density_pred)\n",
    "    print('SMAPE SCORE',error)\n",
    "    return error\n",
    "\n",
    "# check_score(\n",
    "#         X = train_X,\n",
    "#         y_pred=DummyModel().fit(train_X, train_y).predict(train_X), \n",
    "#         y_true=train_y['target_1'])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ad1d413a8513d632047e434ee4038ec414e54bcf3dba008e5fb6dedaf37ce15f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
