{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from feature_engine.outliers import Winsorizer\n",
    "from sklearn.dummy import DummyRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler,MinMaxScaler\n",
    "from feature_engine.outliers import Winsorizer\n",
    "from sklearn.compose import TransformedTargetRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn import set_config, get_config\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer\n",
    "from pprint import pprint\n",
    "from collections import defaultdict\n",
    "import tools\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "set_config(transform_output=\"pandas\")\n",
    "from mlxtend.evaluate.time_series import GroupTimeSeriesSplit, plot_splits, print_cv_info, print_split_info\n",
    "\n",
    "\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.options.display.float_format = '{:.3f}'.format\n",
    "import warnings; warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_train = pd.read_csv('./data/raw/godaddy-microbusiness-density-forecasting_last/revealed_test.csv')\n",
    "# new_train.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_train = pd.read_csv('./data/raw/godaddy-microbusiness-density-forecasting/train.csv')\n",
    "new_train = pd.read_csv('./data/raw/godaddy-microbusiness-density-forecasting_new/revealed_test.csv')\n",
    "\n",
    "old_test = pd.read_csv('./data/raw/godaddy-microbusiness-density-forecasting/test.csv')\n",
    "sample_submission = pd.read_csv('./data/raw/godaddy-microbusiness-density-forecasting/sample_submission.csv')\n",
    "\n",
    "train = pd.concat((old_train, new_train))\n",
    "test = old_test[~old_test['first_day_of_month'].isin(new_train['first_day_of_month'])]\n",
    "\n",
    "train['is_test'] = 0 ; test['is_test'] = 1\n",
    "\n",
    "data = pd.concat((\n",
    "        train,\n",
    "        test)\n",
    "        )\\\n",
    "    .reset_index(drop=True)\\\n",
    "    .assign(\n",
    "        cfips = lambda df: df['cfips'].astype(str).str.zfill(5),\n",
    "        date = lambda df: pd.to_datetime(df[\"first_day_of_month\"]),\n",
    "        # mdensity_t0 = lambda df: df['microbusiness_density'],\n",
    "        # active_t0 = lambda df: df['active'],\n",
    "        )\\\n",
    "    .sort_values(['cfips','date'], ascending=True)\\\n",
    "    .assign(\n",
    "    \n",
    "        state_i = lambda df: df['cfips'].apply(lambda x: x[:2]),\n",
    "        county_i = lambda df: df['cfips'].apply(lambda x: x[2:]),\n",
    "        \n",
    "        year = lambda df: df['date'].dt.year,\n",
    "        date = lambda df: df[\"date\"].dt.date,\n",
    "        # month = lambda df: df['date'].dt.month,\n",
    "\n",
    "        dcount = lambda df: df.groupby('cfips')['row_id'].cumcount(),\n",
    "        \n",
    "        # active_lag1 = lambda df: df.groupby('cfips')['active'].shift(1),\n",
    "        # active_lag2 = lambda df: df.groupby('cfips')['active_t0'].shift(2),\n",
    "        # active_lag3 = lambda df: df.groupby('cfips')['active_t0'].shift(3),\n",
    "        # active_lag_1 = lambda df: df.groupby('cfips')['active'].shift(1),\n",
    "        # active_lag_2 = lambda df: df.groupby('cfips')['active'].shift(2),\n",
    "        active_lag_3 = lambda df: df.groupby('cfips')['active'].shift(3),\n",
    "        active_lag_4 = lambda df: df.groupby('cfips')['active'].shift(4),\n",
    "        active_lag_5 = lambda df: df.groupby('cfips')['active'].shift(5),\n",
    "        active_lag_6 = lambda df: df.groupby('cfips')['active'].shift(6),\n",
    "        active_lag_7 = lambda df: df.groupby('cfips')['active'].shift(7),\n",
    "        active_lag_8 = lambda df: df.groupby('cfips')['active'].shift(8),\n",
    "        active_lag_9 = lambda df: df.groupby('cfips')['active'].shift(9),\n",
    "        active_lag_10 = lambda df: df.groupby('cfips')['active'].shift(10),\n",
    "\n",
    "        target_0 = lambda df: np.nan_to_num(df['active']/df.groupby('cfips')['active'].shift(3)-1, posinf=10),\n",
    "        target_1 = lambda df: np.nan_to_num(df['active']/df.groupby('cfips')['active'].shift(4)-1, posinf=10),\n",
    "        target_2 = lambda df: np.nan_to_num(df['active']/df.groupby('cfips')['active'].shift(5)-1, posinf=10),\n",
    "        # target_0 = lambda df: np.nan_to_num(df['active'], posinf=10),\n",
    "        # target_1 = lambda df: np.nan_to_num(df['active'], posinf=10),\n",
    "        # target_2 = lambda df: np.nan_to_num(df['active'], posinf=10),\n",
    "\n",
    "    )\\\n",
    "    .drop(['county','state'], axis='columns')\n",
    "# .sort_index(ascending=True)\n",
    "\n",
    "assert all(data.groupby('cfips')['county_i'].nunique() == 1)\n",
    "assert all(data.groupby('cfips')['state_i'].nunique() == 1)\n",
    "assert data['cfips'].nunique() == 3135 # there are 3135 county,state tuples\n",
    "assert data['dcount'].nunique() == 47 # there are 47 series for each county state tuple\n",
    "assert data.query('is_test==0')['dcount'].nunique() == 41 # there are 41 series in the train set. \n",
    "assert data.query('is_test==1')['dcount'].nunique() == 6  # there are 6 series in the test set. \n",
    "\n",
    "#The private leaderboard will include 03-2023, 04-2023, 05-2023\n",
    "#The first public leaderboard includes the first month 11-2022.\n",
    "#The updated LB is as 01-2023\n",
    "\n",
    "# adding census data\n",
    "data_census = []\n",
    "for year in range(2017,2022):\n",
    "    COLS = ['GEO_ID','NAME','S0101_C01_026E']\n",
    "    data_census_i = pd.read_csv(f'./data/raw/census_data_1/ACSST5Y{year}.S0101-Data.csv',usecols=COLS)\n",
    "    data_census_i = data_census_i.iloc[1:]\n",
    "    data_census_i['population'] = data_census_i['S0101_C01_026E'].astype('int')\n",
    "\n",
    "\n",
    "    data_census_i['cfips'] = data_census_i.GEO_ID.apply(lambda x: f\"{int(x.split('US')[-1]):05}\" )\n",
    "    data_census_i['year'] = year+2\n",
    "    data_census.append(data_census_i[['cfips','year','population']])\n",
    "\n",
    "data_census = pd.concat((data_census),axis='rows')\n",
    "\n",
    "data = data.merge(data_census, on=['cfips','year'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.query('cfips==\"46127\"')['microbusiness_density'].plot()\n",
    "# data.query('cfips==\"46127\"')['active'].plot()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PARAMETERS\n",
    "n_SPLITS = 4\n",
    "n_TRAIN_TRAIN_SIZE = 6\n",
    "n_TRAIN_PERIOD = n_TRAIN_TRAIN_SIZE  + 3 + n_SPLITS - 1 \n",
    "\n",
    "\n",
    "TEST_DATES = list(np.sort(data.query('is_test==1')['date'].unique())[-4:-1])\n",
    "TEST_PERIOD = list(np.sort(data.query('is_test==1')['dcount'].unique())[-4:-1])\n",
    "\n",
    "TRAIN_DATES = list(np.sort(data.query('is_test==0')['date'].unique())[-n_TRAIN_PERIOD:])\n",
    "TRAIN_PERIOD = list(np.sort(data.query('is_test==0')['dcount'].unique())[-n_TRAIN_PERIOD:])\n",
    "\n",
    "LEAKAGE = ['microbusiness_density','active']\n",
    "TARGETS = ['target_0', 'target_1', 'target_2']\n",
    "FEATURES = ['population']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm \n",
    "\n",
    "sample = data.copy()\n",
    "sample.loc[sample.is_test==1,TARGETS]  = np.nan\n",
    "\n",
    "sample = sample.sort_values(['cfips','date'])\n",
    "sample['bin_log_population'] = pd.cut(np.log1p(sample['population']),bins=8)\n",
    "sample['bin_log_population'] = sample['bin_log_population'].astype(str)\n",
    "LAGS = 8\n",
    "for i in range(3, LAGS+1):\n",
    "    lag_col = f'target_lag{i}'\n",
    "    sample[lag_col] = sample.groupby('cfips')[TARGETS[0]].shift(i)  \n",
    "\n",
    "sample = sample[sample['date'].isin(TRAIN_DATES + TEST_DATES)]\n",
    "sample = sample.sort_values('dcount')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample.query(\"cfips=='56045'\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Individual Features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 51.17it/s]\n"
     ]
    }
   ],
   "source": [
    "TARGET_LAG_LENGTH=8\n",
    "\n",
    "list_target_features = [[f'target_lag{lag_i+model_i+1}' for lag_i in range(2, TARGET_LAG_LENGTH-2)] for model_i in range(3)]\n",
    "\n",
    "\n",
    "tmp_train = np.arange(TARGET_LAG_LENGTH-2-2).reshape((-1,1))\n",
    "tmp_test = np.arange(TARGET_LAG_LENGTH-2-2+1).reshape((-1,1))\n",
    "\n",
    "for model_i in tqdm(range(3)):\n",
    "#     sample[f'target_lr_coef_model_{model_i}'] = sample[list_target_features[model_i]].fillna(0).apply(lambda x: LinearRegression().fit(tmp_train, x).coef_[0], axis='columns')\n",
    "#     sample[f'target_lr_pred_model_{model_i}'] = sample[list_target_features[model_i]].fillna(0).apply(lambda x: LinearRegression().fit(tmp_train, x).predict(tmp_test)[-1], axis='columns')\n",
    "    sample[f'target_mean_lag_model_{model_i}'] = sample[list_target_features[model_i]].mean(axis=1)\n",
    "    sample[f'target_std_lag_model_{model_i}'] = sample[list_target_features[model_i]].std(axis=1)\n",
    "    sample[f'target_median_lag_model_{model_i}'] = sample[list_target_features[model_i]].median(axis=1)\n",
    "    list_target_features[model_i].extend([f'target_mean_lag_model_{model_i}',f'target_std_lag_model_{model_i}',f'target_median_lag_model_{model_i}'])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample.to_pickle('./data/sample_v1.p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample = pd.read_pickle('./data/sample_v1.p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ran_id = np.random.randint(200)\n",
    "# plt.plot(sample[list_cols_model[0]+['mean_lag_0']].iloc[ran_id] )\n",
    "# plt.plot(sample[list_cols_model[0]+['lr_pred_0']].iloc[ran_id], '--' )\n",
    "# plt.plot(sample[list_cols_model[0]+['median_lag_0']].iloc[ran_id], '--' )\n",
    "# plt.plot(sample[list_cols_model[0]].iloc[ran_id], color='r' )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hiearchy features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 46.03it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 46.56it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 47.57it/s]\n"
     ]
    }
   ],
   "source": [
    "# county, date\n",
    "hierarchy_categories = ['bin_log_population', 'county_i', 'state_i']\n",
    "for VAR_HIERARCHY in hierarchy_categories:\n",
    "    TARGET_LAG_LENGTH=8\n",
    "\n",
    "\n",
    "    df_hierarchy = sample.groupby([VAR_HIERARCHY,'date'])[[f'active_lag_{i}' for i in range(3,TARGET_LAG_LENGTH+3)]].sum()\n",
    "    df_hierarchy = df_hierarchy.replace(0, np.nan)\n",
    "\n",
    "\n",
    "    for i in range(2,TARGET_LAG_LENGTH):\n",
    "        df_hierarchy[f'{VAR_HIERARCHY}_target_lag_{i+1}'] = (df_hierarchy[f'active_lag_{i+1}']/df_hierarchy[f'active_lag_{i+2}'])-1\n",
    "\n",
    "    list_hierarchy_features = [[f'{VAR_HIERARCHY}_target_lag_{lag_i+model_i+1}' for lag_i in range(2, TARGET_LAG_LENGTH-2)] for model_i in range(3)]\n",
    "\n",
    "    tmp_train = np.arange(TARGET_LAG_LENGTH-2-2).reshape((-1,1))\n",
    "    tmp_test = np.arange(TARGET_LAG_LENGTH-2-2+1).reshape((-1,1))\n",
    "\n",
    "    for model_i in tqdm(range(3)):\n",
    "        # df_hierarchy[f'hierarchy_{VAR_HIERARCHY}_lr_coef_model_{model_i}'] = df_hierarchy[list_hierarchy_features[model_i]].fillna(0).apply(lambda x: LinearRegression().fit(tmp_train, x).coef_[0], axis='columns')\n",
    "        # df_hierarchy[f'hierarchy_{VAR_HIERARCHY}_lr_pred_model_{model_i}'] = df_hierarchy[list_hierarchy_features[model_i]].fillna(0).apply(lambda x: LinearRegression().fit(tmp_train, x).predict(tmp_test)[-1], axis='columns')\n",
    "        df_hierarchy[f'hierarchy_{VAR_HIERARCHY}_mean_lag_model_{model_i}'] = df_hierarchy[list_hierarchy_features[model_i]].mean(axis=1)\n",
    "        df_hierarchy[f'hierarchy_{VAR_HIERARCHY}_std_lag_model_{model_i}'] = df_hierarchy[list_hierarchy_features[model_i]].std(axis=1)\n",
    "        df_hierarchy[f'hierarchy_{VAR_HIERARCHY}_median_lag_model_{model_i}'] = df_hierarchy[list_hierarchy_features[model_i]].median(axis=1)\n",
    "\n",
    "        df_hierarchy\n",
    "        sample = sample.merge(df_hierarchy[[\n",
    "            # f'hierarchy_{VAR_HIERARCHY}_lr_coef_model_{model_i}',\n",
    "            # f'hierarchy_{VAR_HIERARCHY}_lr_pred_model_{model_i}',\n",
    "            f'hierarchy_{VAR_HIERARCHY}_mean_lag_model_{model_i}',\n",
    "            f'hierarchy_{VAR_HIERARCHY}_std_lag_model_{model_i}',\n",
    "            f'hierarchy_{VAR_HIERARCHY}_median_lag_model_{model_i}'\n",
    "        ]].reset_index(),\n",
    "        on = [f'{VAR_HIERARCHY}','date'],\n",
    "        how='left'\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_hierarchy_features = []\n",
    "for model_i in range(3):\n",
    "    tmp =[]\n",
    "    for col in sample.columns:\n",
    "        if (col.split('_')[0] == 'hierarchy') and (col.split('_')[-1] == f'{model_i}'):    \n",
    "          tmp.append(col)\n",
    "    list_hierarchy_features.append(tmp)\n",
    "# hierarchy_features  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ran_id = np.random.randint(len(df_hierarchy))\n",
    "# plt.plot(df_hierarchy[list_cols_model[0]+[f'{VAR_HIERARCHY}_mean_lag_0']].iloc[ran_id] )\n",
    "# plt.plot(df_hierarchy[list_cols_model[0]+[f'{VAR_HIERARCHY}_lr_pred_0']].iloc[ran_id], '--' )\n",
    "# plt.plot(df_hierarchy[list_cols_model[0]+[f'{VAR_HIERARCHY}_median_lag_0']].iloc[ran_id], '--' )\n",
    "# plt.plot(df_hierarchy[list_cols_model[0]].iloc[ran_id] )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display(\n",
    "# train_X.head(2),\n",
    "# # train_X.tail(2),\n",
    "# # test_X.head(2),\n",
    "# # test_X.tail(2)\n",
    "# )\n",
    "# list_cols_model[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import KBinsDiscretizer, LabelEncoder, QuantileTransformer, OneHotEncoder\n",
    "from ray.tune.sklearn import TuneGridSearchCV\n",
    "from ray.tune.sklearn import TuneSearchCV\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from category_encoders.target_encoder import TargetEncoder\n",
    "\n",
    "from tools import BaseTransformer\n",
    "\n",
    "class CategoricalFeatureEngineering(BaseTransformer):\n",
    "    def __init__(self, features=None):\n",
    "        self.features = features\n",
    "\t    \n",
    "    def fit(self, X, y=None):\n",
    "        self.dict_transformers = {}\n",
    "        \n",
    "        self.dict_transformers['le_state_i'] = TargetEncoder(handle_unknown=np.nan).fit(X['state_i'],y)\n",
    "        self.dict_transformers['le_county_i'] = TargetEncoder(handle_unknown=np.nan).fit(X['county_i'],y)\n",
    "        self.dict_transformers['le_bin_log_population'] = TargetEncoder(handle_unknown=np.nan).fit(X['bin_log_population'],y)\n",
    "\n",
    "\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        X_transformed = X.copy()\n",
    "        X_transformed['le_state_i'] = self.dict_transformers['le_state_i'].transform(X_transformed['state_i'])\n",
    "        X_transformed['le_county_i'] =  self.dict_transformers['le_county_i'].transform(X_transformed['county_i'])\n",
    "        X_transformed['le_bin_log_population'] =  self.dict_transformers['le_bin_log_population'].transform(X_transformed['bin_log_population'])\n",
    "\n",
    "        return X_transformed[['le_state_i','le_county_i','le_bin_log_population']]\n",
    "\n",
    "# CategoricalFeatureEngineering().fit_transform(train_X, train_X['target_0'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipelining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_train= sample.query(\"dcount in @TRAIN_PERIOD\") ; sample_test= sample.query(\"dcount in @TEST_PERIOD\")\n",
    "train_y = sample_train[TARGETS] ; test_y = sample_test[TARGETS]\n",
    "# train_X = sample_train.drop(TARGETS,axis='columns')\n",
    "train_X = sample_train ; test_X = sample_test\n",
    "# test_X = sample_test.drop(TARGETS,axis='columns')  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\n",
    "import xgboost as xgb\n",
    "from sklearn.linear_model import Ridge, Lasso\n",
    "from sklearn.svm import SVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Users/samet/.pyenv/versions/3.9.15/envs/fistik/lib/python3.9/site-packages/joblib/parallel.py\", line 862, in dispatch_one_batch\n",
      "    tasks = self._ready_batches.get(block=False)\n",
      "  File \"/Users/samet/.pyenv/versions/3.9.15/lib/python3.9/queue.py\", line 168, in get\n",
      "    raise Empty\n",
      "_queue.Empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/samet/.pyenv/versions/3.9.15/envs/fistik/lib/python3.9/site-packages/debugpy/_vendored/pydevd/_pydevd_bundle/pydevd_frame.py\", line 987, in trace_dispatch\n",
      "    self.do_wait_suspend(thread, frame, event, arg)\n",
      "  File \"/Users/samet/.pyenv/versions/3.9.15/envs/fistik/lib/python3.9/site-packages/debugpy/_vendored/pydevd/_pydevd_bundle/pydevd_frame.py\", line 164, in do_wait_suspend\n",
      "    self._args[0].do_wait_suspend(*args, **kwargs)\n",
      "  File \"/Users/samet/.pyenv/versions/3.9.15/envs/fistik/lib/python3.9/site-packages/debugpy/_vendored/pydevd/pydevd.py\", line 2062, in do_wait_suspend\n",
      "    keep_suspended = self._do_wait_suspend(thread, frame, event, arg, suspend_type, from_this_thread, frames_tracker)\n",
      "  File \"/Users/samet/.pyenv/versions/3.9.15/envs/fistik/lib/python3.9/site-packages/debugpy/_vendored/pydevd/pydevd.py\", line 2098, in _do_wait_suspend\n",
      "    time.sleep(0.01)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mEmpty\u001b[0m                                     Traceback (most recent call last)",
      "File \u001b[0;32m~/.pyenv/versions/3.9.15/envs/fistik/lib/python3.9/site-packages/joblib/parallel.py:862\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    861\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 862\u001b[0m     tasks \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_ready_batches\u001b[39m.\u001b[39;49mget(block\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m    863\u001b[0m \u001b[39mexcept\u001b[39;00m queue\u001b[39m.\u001b[39mEmpty:\n\u001b[1;32m    864\u001b[0m     \u001b[39m# slice the iterator n_jobs * batchsize items at a time. If the\u001b[39;00m\n\u001b[1;32m    865\u001b[0m     \u001b[39m# slice returns less than that, then the current batchsize puts\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    868\u001b[0m     \u001b[39m# accordingly to distribute evenly the last items between all\u001b[39;00m\n\u001b[1;32m    869\u001b[0m     \u001b[39m# workers.\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.15/lib/python3.9/queue.py:168\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    167\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_qsize():\n\u001b[0;32m--> 168\u001b[0m         \u001b[39mraise\u001b[39;00m Empty\n\u001b[1;32m    169\u001b[0m \u001b[39melif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mEmpty\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:87\u001b[0m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.15/envs/fistik/lib/python3.9/site-packages/sklearn/model_selection/_search.py:875\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    869\u001b[0m     results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_results(\n\u001b[1;32m    870\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[1;32m    871\u001b[0m     )\n\u001b[1;32m    873\u001b[0m     \u001b[39mreturn\u001b[39;00m results\n\u001b[0;32m--> 875\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_search(evaluate_candidates)\n\u001b[1;32m    877\u001b[0m \u001b[39m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[1;32m    878\u001b[0m \u001b[39m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[1;32m    879\u001b[0m first_test_score \u001b[39m=\u001b[39m all_out[\u001b[39m0\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mtest_scores\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.15/envs/fistik/lib/python3.9/site-packages/sklearn/model_selection/_search.py:1769\u001b[0m, in \u001b[0;36mRandomizedSearchCV._run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1767\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_run_search\u001b[39m(\u001b[39mself\u001b[39m, evaluate_candidates):\n\u001b[1;32m   1768\u001b[0m     \u001b[39m\"\"\"Search n_iter candidates from param_distributions\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1769\u001b[0m     evaluate_candidates(\n\u001b[1;32m   1770\u001b[0m         ParameterSampler(\n\u001b[1;32m   1771\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparam_distributions, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mn_iter, random_state\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrandom_state\n\u001b[1;32m   1772\u001b[0m         )\n\u001b[1;32m   1773\u001b[0m     )\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.15/envs/fistik/lib/python3.9/site-packages/sklearn/model_selection/_search.py:822\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    814\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    815\u001b[0m     \u001b[39mprint\u001b[39m(\n\u001b[1;32m    816\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mFitting \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m folds for each of \u001b[39m\u001b[39m{1}\u001b[39;00m\u001b[39m candidates,\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    817\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m totalling \u001b[39m\u001b[39m{2}\u001b[39;00m\u001b[39m fits\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m    818\u001b[0m             n_splits, n_candidates, n_candidates \u001b[39m*\u001b[39m n_splits\n\u001b[1;32m    819\u001b[0m         )\n\u001b[1;32m    820\u001b[0m     )\n\u001b[0;32m--> 822\u001b[0m out \u001b[39m=\u001b[39m parallel(\n\u001b[1;32m    823\u001b[0m     delayed(_fit_and_score)(\n\u001b[1;32m    824\u001b[0m         clone(base_estimator),\n\u001b[1;32m    825\u001b[0m         X,\n\u001b[1;32m    826\u001b[0m         y,\n\u001b[1;32m    827\u001b[0m         train\u001b[39m=\u001b[39;49mtrain,\n\u001b[1;32m    828\u001b[0m         test\u001b[39m=\u001b[39;49mtest,\n\u001b[1;32m    829\u001b[0m         parameters\u001b[39m=\u001b[39;49mparameters,\n\u001b[1;32m    830\u001b[0m         split_progress\u001b[39m=\u001b[39;49m(split_idx, n_splits),\n\u001b[1;32m    831\u001b[0m         candidate_progress\u001b[39m=\u001b[39;49m(cand_idx, n_candidates),\n\u001b[1;32m    832\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_and_score_kwargs,\n\u001b[1;32m    833\u001b[0m     )\n\u001b[1;32m    834\u001b[0m     \u001b[39mfor\u001b[39;49;00m (cand_idx, parameters), (split_idx, (train, test)) \u001b[39min\u001b[39;49;00m product(\n\u001b[1;32m    835\u001b[0m         \u001b[39menumerate\u001b[39;49m(candidate_params), \u001b[39menumerate\u001b[39;49m(cv\u001b[39m.\u001b[39;49msplit(X, y, groups))\n\u001b[1;32m    836\u001b[0m     )\n\u001b[1;32m    837\u001b[0m )\n\u001b[1;32m    839\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(out) \u001b[39m<\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m    840\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    841\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mNo fits were performed. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    842\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mWas the CV iterator empty? \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    843\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mWere there no candidates?\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    844\u001b[0m     )\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.15/envs/fistik/lib/python3.9/site-packages/joblib/parallel.py:1088\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1085\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdispatch_one_batch(iterator):\n\u001b[1;32m   1086\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterating \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_original_iterator \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m-> 1088\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdispatch_one_batch(iterator):\n\u001b[1;32m   1089\u001b[0m     \u001b[39mpass\u001b[39;00m\n\u001b[1;32m   1091\u001b[0m \u001b[39mif\u001b[39;00m pre_dispatch \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mall\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mor\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m   1092\u001b[0m     \u001b[39m# The iterable was consumed all at once by the above for loop.\u001b[39;00m\n\u001b[1;32m   1093\u001b[0m     \u001b[39m# No need to wait for async callbacks to trigger to\u001b[39;00m\n\u001b[1;32m   1094\u001b[0m     \u001b[39m# consumption.\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.15/envs/fistik/lib/python3.9/site-packages/joblib/parallel.py:873\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    870\u001b[0m n_jobs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_cached_effective_n_jobs\n\u001b[1;32m    871\u001b[0m big_batch_size \u001b[39m=\u001b[39m batch_size \u001b[39m*\u001b[39m n_jobs\n\u001b[0;32m--> 873\u001b[0m islice \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39;49m(itertools\u001b[39m.\u001b[39;49mislice(iterator, big_batch_size))\n\u001b[1;32m    874\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(islice) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    875\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.15/envs/fistik/lib/python3.9/site-packages/sklearn/model_selection/_search.py:824\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    814\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    815\u001b[0m     \u001b[39mprint\u001b[39m(\n\u001b[1;32m    816\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mFitting \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m folds for each of \u001b[39m\u001b[39m{1}\u001b[39;00m\u001b[39m candidates,\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    817\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m totalling \u001b[39m\u001b[39m{2}\u001b[39;00m\u001b[39m fits\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m    818\u001b[0m             n_splits, n_candidates, n_candidates \u001b[39m*\u001b[39m n_splits\n\u001b[1;32m    819\u001b[0m         )\n\u001b[1;32m    820\u001b[0m     )\n\u001b[1;32m    822\u001b[0m out \u001b[39m=\u001b[39m parallel(\n\u001b[1;32m    823\u001b[0m     delayed(_fit_and_score)(\n\u001b[0;32m--> 824\u001b[0m         clone(base_estimator),\n\u001b[1;32m    825\u001b[0m         X,\n\u001b[1;32m    826\u001b[0m         y,\n\u001b[1;32m    827\u001b[0m         train\u001b[39m=\u001b[39mtrain,\n\u001b[1;32m    828\u001b[0m         test\u001b[39m=\u001b[39mtest,\n\u001b[1;32m    829\u001b[0m         parameters\u001b[39m=\u001b[39mparameters,\n\u001b[1;32m    830\u001b[0m         split_progress\u001b[39m=\u001b[39m(split_idx, n_splits),\n\u001b[1;32m    831\u001b[0m         candidate_progress\u001b[39m=\u001b[39m(cand_idx, n_candidates),\n\u001b[1;32m    832\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_and_score_kwargs,\n\u001b[1;32m    833\u001b[0m     )\n\u001b[1;32m    834\u001b[0m     \u001b[39mfor\u001b[39;00m (cand_idx, parameters), (split_idx, (train, test)) \u001b[39min\u001b[39;00m product(\n\u001b[1;32m    835\u001b[0m         \u001b[39menumerate\u001b[39m(candidate_params), \u001b[39menumerate\u001b[39m(cv\u001b[39m.\u001b[39msplit(X, y, groups))\n\u001b[1;32m    836\u001b[0m     )\n\u001b[1;32m    837\u001b[0m )\n\u001b[1;32m    839\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(out) \u001b[39m<\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m    840\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    841\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mNo fits were performed. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    842\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mWas the CV iterator empty? \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    843\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mWere there no candidates?\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    844\u001b[0m     )\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.15/envs/fistik/lib/python3.9/site-packages/sklearn/base.py:89\u001b[0m, in \u001b[0;36mclone\u001b[0;34m(estimator, safe)\u001b[0m\n\u001b[1;32m     87\u001b[0m new_object_params \u001b[39m=\u001b[39m estimator\u001b[39m.\u001b[39mget_params(deep\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m     88\u001b[0m \u001b[39mfor\u001b[39;00m name, param \u001b[39min\u001b[39;00m new_object_params\u001b[39m.\u001b[39mitems():\n\u001b[0;32m---> 89\u001b[0m     new_object_params[name] \u001b[39m=\u001b[39m clone(param, safe\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m     90\u001b[0m new_object \u001b[39m=\u001b[39m klass(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mnew_object_params)\n\u001b[1;32m     91\u001b[0m params_set \u001b[39m=\u001b[39m new_object\u001b[39m.\u001b[39mget_params(deep\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.15/envs/fistik/lib/python3.9/site-packages/sklearn/base.py:67\u001b[0m, in \u001b[0;36mclone\u001b[0;34m(estimator, safe)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[39m# XXX: not handling dictionaries\u001b[39;00m\n\u001b[1;32m     66\u001b[0m \u001b[39mif\u001b[39;00m estimator_type \u001b[39min\u001b[39;00m (\u001b[39mlist\u001b[39m, \u001b[39mtuple\u001b[39m, \u001b[39mset\u001b[39m, \u001b[39mfrozenset\u001b[39m):\n\u001b[0;32m---> 67\u001b[0m     \u001b[39mreturn\u001b[39;00m estimator_type([clone(e, safe\u001b[39m=\u001b[39msafe) \u001b[39mfor\u001b[39;00m e \u001b[39min\u001b[39;00m estimator])\n\u001b[1;32m     68\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mhasattr\u001b[39m(estimator, \u001b[39m\"\u001b[39m\u001b[39mget_params\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mor\u001b[39;00m \u001b[39misinstance\u001b[39m(estimator, \u001b[39mtype\u001b[39m):\n\u001b[1;32m     69\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m safe:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.15/envs/fistik/lib/python3.9/site-packages/sklearn/base.py:67\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[39m# XXX: not handling dictionaries\u001b[39;00m\n\u001b[1;32m     66\u001b[0m \u001b[39mif\u001b[39;00m estimator_type \u001b[39min\u001b[39;00m (\u001b[39mlist\u001b[39m, \u001b[39mtuple\u001b[39m, \u001b[39mset\u001b[39m, \u001b[39mfrozenset\u001b[39m):\n\u001b[0;32m---> 67\u001b[0m     \u001b[39mreturn\u001b[39;00m estimator_type([clone(e, safe\u001b[39m=\u001b[39;49msafe) \u001b[39mfor\u001b[39;00m e \u001b[39min\u001b[39;00m estimator])\n\u001b[1;32m     68\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mhasattr\u001b[39m(estimator, \u001b[39m\"\u001b[39m\u001b[39mget_params\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mor\u001b[39;00m \u001b[39misinstance\u001b[39m(estimator, \u001b[39mtype\u001b[39m):\n\u001b[1;32m     69\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m safe:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.15/envs/fistik/lib/python3.9/site-packages/sklearn/base.py:67\u001b[0m, in \u001b[0;36mclone\u001b[0;34m(estimator, safe)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[39m# XXX: not handling dictionaries\u001b[39;00m\n\u001b[1;32m     66\u001b[0m \u001b[39mif\u001b[39;00m estimator_type \u001b[39min\u001b[39;00m (\u001b[39mlist\u001b[39m, \u001b[39mtuple\u001b[39m, \u001b[39mset\u001b[39m, \u001b[39mfrozenset\u001b[39m):\n\u001b[0;32m---> 67\u001b[0m     \u001b[39mreturn\u001b[39;00m estimator_type([clone(e, safe\u001b[39m=\u001b[39msafe) \u001b[39mfor\u001b[39;00m e \u001b[39min\u001b[39;00m estimator])\n\u001b[1;32m     68\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mhasattr\u001b[39m(estimator, \u001b[39m\"\u001b[39m\u001b[39mget_params\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mor\u001b[39;00m \u001b[39misinstance\u001b[39m(estimator, \u001b[39mtype\u001b[39m):\n\u001b[1;32m     69\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m safe:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.15/envs/fistik/lib/python3.9/site-packages/sklearn/base.py:67\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[39m# XXX: not handling dictionaries\u001b[39;00m\n\u001b[1;32m     66\u001b[0m \u001b[39mif\u001b[39;00m estimator_type \u001b[39min\u001b[39;00m (\u001b[39mlist\u001b[39m, \u001b[39mtuple\u001b[39m, \u001b[39mset\u001b[39m, \u001b[39mfrozenset\u001b[39m):\n\u001b[0;32m---> 67\u001b[0m     \u001b[39mreturn\u001b[39;00m estimator_type([clone(e, safe\u001b[39m=\u001b[39;49msafe) \u001b[39mfor\u001b[39;00m e \u001b[39min\u001b[39;00m estimator])\n\u001b[1;32m     68\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mhasattr\u001b[39m(estimator, \u001b[39m\"\u001b[39m\u001b[39mget_params\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mor\u001b[39;00m \u001b[39misinstance\u001b[39m(estimator, \u001b[39mtype\u001b[39m):\n\u001b[1;32m     69\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m safe:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.15/envs/fistik/lib/python3.9/site-packages/sklearn/base.py:89\u001b[0m, in \u001b[0;36mclone\u001b[0;34m(estimator, safe)\u001b[0m\n\u001b[1;32m     87\u001b[0m new_object_params \u001b[39m=\u001b[39m estimator\u001b[39m.\u001b[39mget_params(deep\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m     88\u001b[0m \u001b[39mfor\u001b[39;00m name, param \u001b[39min\u001b[39;00m new_object_params\u001b[39m.\u001b[39mitems():\n\u001b[0;32m---> 89\u001b[0m     new_object_params[name] \u001b[39m=\u001b[39m clone(param, safe\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m     90\u001b[0m new_object \u001b[39m=\u001b[39m klass(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mnew_object_params)\n\u001b[1;32m     91\u001b[0m params_set \u001b[39m=\u001b[39m new_object\u001b[39m.\u001b[39mget_params(deep\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.15/envs/fistik/lib/python3.9/site-packages/sklearn/base.py:67\u001b[0m, in \u001b[0;36mclone\u001b[0;34m(estimator, safe)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[39m# XXX: not handling dictionaries\u001b[39;00m\n\u001b[1;32m     66\u001b[0m \u001b[39mif\u001b[39;00m estimator_type \u001b[39min\u001b[39;00m (\u001b[39mlist\u001b[39m, \u001b[39mtuple\u001b[39m, \u001b[39mset\u001b[39m, \u001b[39mfrozenset\u001b[39m):\n\u001b[0;32m---> 67\u001b[0m     \u001b[39mreturn\u001b[39;00m estimator_type([clone(e, safe\u001b[39m=\u001b[39msafe) \u001b[39mfor\u001b[39;00m e \u001b[39min\u001b[39;00m estimator])\n\u001b[1;32m     68\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mhasattr\u001b[39m(estimator, \u001b[39m\"\u001b[39m\u001b[39mget_params\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mor\u001b[39;00m \u001b[39misinstance\u001b[39m(estimator, \u001b[39mtype\u001b[39m):\n\u001b[1;32m     69\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m safe:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.15/envs/fistik/lib/python3.9/site-packages/sklearn/base.py:67\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[39m# XXX: not handling dictionaries\u001b[39;00m\n\u001b[1;32m     66\u001b[0m \u001b[39mif\u001b[39;00m estimator_type \u001b[39min\u001b[39;00m (\u001b[39mlist\u001b[39m, \u001b[39mtuple\u001b[39m, \u001b[39mset\u001b[39m, \u001b[39mfrozenset\u001b[39m):\n\u001b[0;32m---> 67\u001b[0m     \u001b[39mreturn\u001b[39;00m estimator_type([clone(e, safe\u001b[39m=\u001b[39;49msafe) \u001b[39mfor\u001b[39;00m e \u001b[39min\u001b[39;00m estimator])\n\u001b[1;32m     68\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mhasattr\u001b[39m(estimator, \u001b[39m\"\u001b[39m\u001b[39mget_params\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mor\u001b[39;00m \u001b[39misinstance\u001b[39m(estimator, \u001b[39mtype\u001b[39m):\n\u001b[1;32m     69\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m safe:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.15/envs/fistik/lib/python3.9/site-packages/sklearn/base.py:67\u001b[0m, in \u001b[0;36mclone\u001b[0;34m(estimator, safe)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[39m# XXX: not handling dictionaries\u001b[39;00m\n\u001b[1;32m     66\u001b[0m \u001b[39mif\u001b[39;00m estimator_type \u001b[39min\u001b[39;00m (\u001b[39mlist\u001b[39m, \u001b[39mtuple\u001b[39m, \u001b[39mset\u001b[39m, \u001b[39mfrozenset\u001b[39m):\n\u001b[0;32m---> 67\u001b[0m     \u001b[39mreturn\u001b[39;00m estimator_type([clone(e, safe\u001b[39m=\u001b[39msafe) \u001b[39mfor\u001b[39;00m e \u001b[39min\u001b[39;00m estimator])\n\u001b[1;32m     68\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mhasattr\u001b[39m(estimator, \u001b[39m\"\u001b[39m\u001b[39mget_params\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mor\u001b[39;00m \u001b[39misinstance\u001b[39m(estimator, \u001b[39mtype\u001b[39m):\n\u001b[1;32m     69\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m safe:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.15/envs/fistik/lib/python3.9/site-packages/sklearn/base.py:67\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[39m# XXX: not handling dictionaries\u001b[39;00m\n\u001b[1;32m     66\u001b[0m \u001b[39mif\u001b[39;00m estimator_type \u001b[39min\u001b[39;00m (\u001b[39mlist\u001b[39m, \u001b[39mtuple\u001b[39m, \u001b[39mset\u001b[39m, \u001b[39mfrozenset\u001b[39m):\n\u001b[0;32m---> 67\u001b[0m     \u001b[39mreturn\u001b[39;00m estimator_type([clone(e, safe\u001b[39m=\u001b[39;49msafe) \u001b[39mfor\u001b[39;00m e \u001b[39min\u001b[39;00m estimator])\n\u001b[1;32m     68\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mhasattr\u001b[39m(estimator, \u001b[39m\"\u001b[39m\u001b[39mget_params\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mor\u001b[39;00m \u001b[39misinstance\u001b[39m(estimator, \u001b[39mtype\u001b[39m):\n\u001b[1;32m     69\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m safe:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.15/envs/fistik/lib/python3.9/site-packages/sklearn/base.py:89\u001b[0m, in \u001b[0;36mclone\u001b[0;34m(estimator, safe)\u001b[0m\n\u001b[1;32m     87\u001b[0m new_object_params \u001b[39m=\u001b[39m estimator\u001b[39m.\u001b[39mget_params(deep\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m     88\u001b[0m \u001b[39mfor\u001b[39;00m name, param \u001b[39min\u001b[39;00m new_object_params\u001b[39m.\u001b[39mitems():\n\u001b[0;32m---> 89\u001b[0m     new_object_params[name] \u001b[39m=\u001b[39m clone(param, safe\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m     90\u001b[0m new_object \u001b[39m=\u001b[39m klass(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mnew_object_params)\n\u001b[1;32m     91\u001b[0m params_set \u001b[39m=\u001b[39m new_object\u001b[39m.\u001b[39mget_params(deep\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.15/envs/fistik/lib/python3.9/site-packages/sklearn/base.py:67\u001b[0m, in \u001b[0;36mclone\u001b[0;34m(estimator, safe)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[39m# XXX: not handling dictionaries\u001b[39;00m\n\u001b[1;32m     66\u001b[0m \u001b[39mif\u001b[39;00m estimator_type \u001b[39min\u001b[39;00m (\u001b[39mlist\u001b[39m, \u001b[39mtuple\u001b[39m, \u001b[39mset\u001b[39m, \u001b[39mfrozenset\u001b[39m):\n\u001b[0;32m---> 67\u001b[0m     \u001b[39mreturn\u001b[39;00m estimator_type([clone(e, safe\u001b[39m=\u001b[39msafe) \u001b[39mfor\u001b[39;00m e \u001b[39min\u001b[39;00m estimator])\n\u001b[1;32m     68\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mhasattr\u001b[39m(estimator, \u001b[39m\"\u001b[39m\u001b[39mget_params\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mor\u001b[39;00m \u001b[39misinstance\u001b[39m(estimator, \u001b[39mtype\u001b[39m):\n\u001b[1;32m     69\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m safe:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.15/envs/fistik/lib/python3.9/site-packages/sklearn/base.py:67\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[39m# XXX: not handling dictionaries\u001b[39;00m\n\u001b[1;32m     66\u001b[0m \u001b[39mif\u001b[39;00m estimator_type \u001b[39min\u001b[39;00m (\u001b[39mlist\u001b[39m, \u001b[39mtuple\u001b[39m, \u001b[39mset\u001b[39m, \u001b[39mfrozenset\u001b[39m):\n\u001b[0;32m---> 67\u001b[0m     \u001b[39mreturn\u001b[39;00m estimator_type([clone(e, safe\u001b[39m=\u001b[39;49msafe) \u001b[39mfor\u001b[39;00m e \u001b[39min\u001b[39;00m estimator])\n\u001b[1;32m     68\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mhasattr\u001b[39m(estimator, \u001b[39m\"\u001b[39m\u001b[39mget_params\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mor\u001b[39;00m \u001b[39misinstance\u001b[39m(estimator, \u001b[39mtype\u001b[39m):\n\u001b[1;32m     69\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m safe:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.15/envs/fistik/lib/python3.9/site-packages/sklearn/base.py:67\u001b[0m, in \u001b[0;36mclone\u001b[0;34m(estimator, safe)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[39m# XXX: not handling dictionaries\u001b[39;00m\n\u001b[1;32m     66\u001b[0m \u001b[39mif\u001b[39;00m estimator_type \u001b[39min\u001b[39;00m (\u001b[39mlist\u001b[39m, \u001b[39mtuple\u001b[39m, \u001b[39mset\u001b[39m, \u001b[39mfrozenset\u001b[39m):\n\u001b[0;32m---> 67\u001b[0m     \u001b[39mreturn\u001b[39;00m estimator_type([clone(e, safe\u001b[39m=\u001b[39msafe) \u001b[39mfor\u001b[39;00m e \u001b[39min\u001b[39;00m estimator])\n\u001b[1;32m     68\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mhasattr\u001b[39m(estimator, \u001b[39m\"\u001b[39m\u001b[39mget_params\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mor\u001b[39;00m \u001b[39misinstance\u001b[39m(estimator, \u001b[39mtype\u001b[39m):\n\u001b[1;32m     69\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m safe:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.15/envs/fistik/lib/python3.9/site-packages/sklearn/base.py:67\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[39m# XXX: not handling dictionaries\u001b[39;00m\n\u001b[1;32m     66\u001b[0m \u001b[39mif\u001b[39;00m estimator_type \u001b[39min\u001b[39;00m (\u001b[39mlist\u001b[39m, \u001b[39mtuple\u001b[39m, \u001b[39mset\u001b[39m, \u001b[39mfrozenset\u001b[39m):\n\u001b[0;32m---> 67\u001b[0m     \u001b[39mreturn\u001b[39;00m estimator_type([clone(e, safe\u001b[39m=\u001b[39;49msafe) \u001b[39mfor\u001b[39;00m e \u001b[39min\u001b[39;00m estimator])\n\u001b[1;32m     68\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mhasattr\u001b[39m(estimator, \u001b[39m\"\u001b[39m\u001b[39mget_params\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mor\u001b[39;00m \u001b[39misinstance\u001b[39m(estimator, \u001b[39mtype\u001b[39m):\n\u001b[1;32m     69\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m safe:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.15/envs/fistik/lib/python3.9/site-packages/sklearn/base.py:90\u001b[0m, in \u001b[0;36mclone\u001b[0;34m(estimator, safe)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[39mfor\u001b[39;00m name, param \u001b[39min\u001b[39;00m new_object_params\u001b[39m.\u001b[39mitems():\n\u001b[1;32m     89\u001b[0m     new_object_params[name] \u001b[39m=\u001b[39m clone(param, safe\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m---> 90\u001b[0m new_object \u001b[39m=\u001b[39m klass(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mnew_object_params)\n\u001b[1;32m     91\u001b[0m params_set \u001b[39m=\u001b[39m new_object\u001b[39m.\u001b[39mget_params(deep\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m     93\u001b[0m \u001b[39m# quick sanity check of the parameters of the clone\u001b[39;00m\n",
      "File \u001b[0;32m~/projects/kaggle_GoDaddy/tools.py:10\u001b[0m, in \u001b[0;36mColumnSelector.__init__\u001b[0;34m(self, features)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mColumnSelector\u001b[39;00m(BaseEstimator, TransformerMixin):\n\u001b[1;32m      9\u001b[0m     \u001b[39m\"\"\"Select only specified columns.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, features):\n\u001b[1;32m     11\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfeatures \u001b[39m=\u001b[39m features\n\u001b[1;32m     13\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mfit\u001b[39m(\u001b[39mself\u001b[39m, X, y\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.15/envs/fistik/lib/python3.9/site-packages/debugpy/_vendored/pydevd/_pydevd_bundle/pydevd_trace_dispatch_regular.py:436\u001b[0m, in \u001b[0;36mThreadTracer.__call__\u001b[0;34m(self, frame, event, arg)\u001b[0m\n\u001b[1;32m    430\u001b[0m             \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mif\u001b[39;00m event \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mcall\u001b[39m\u001b[39m'\u001b[39m \u001b[39melse\u001b[39;00m NO_FTRACE\n\u001b[1;32m    432\u001b[0m \u001b[39m# if DEBUG: print('trace_dispatch', filename, frame.f_lineno, event, frame.f_code.co_name, file_type)\u001b[39;00m\n\u001b[1;32m    433\u001b[0m \n\u001b[1;32m    434\u001b[0m \u001b[39m# Just create PyDBFrame directly (removed support for Python versions < 2.5, which required keeping a weak\u001b[39;00m\n\u001b[1;32m    435\u001b[0m \u001b[39m# reference to the frame).\u001b[39;00m\n\u001b[0;32m--> 436\u001b[0m ret \u001b[39m=\u001b[39m PyDBFrame(\n\u001b[1;32m    437\u001b[0m     (\n\u001b[1;32m    438\u001b[0m         py_db, abs_path_canonical_path_and_base, additional_info, t, frame_skips_cache, frame_cache_key,\n\u001b[1;32m    439\u001b[0m     )\n\u001b[1;32m    440\u001b[0m )\u001b[39m.\u001b[39;49mtrace_dispatch(frame, event, arg)\n\u001b[1;32m    441\u001b[0m \u001b[39mif\u001b[39;00m ret \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    442\u001b[0m     \u001b[39m# 1 means skipped because of filters.\u001b[39;00m\n\u001b[1;32m    443\u001b[0m     \u001b[39m# 2 means skipped because no breakpoints were hit.\u001b[39;00m\n\u001b[1;32m    444\u001b[0m     cache_skips[frame_cache_key] \u001b[39m=\u001b[39m \u001b[39m2\u001b[39m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.15/envs/fistik/lib/python3.9/site-packages/debugpy/_vendored/pydevd/_pydevd_bundle/pydevd_frame.py:987\u001b[0m, in \u001b[0;36mPyDBFrame.trace_dispatch\u001b[0;34m(self, frame, event, arg)\u001b[0m\n\u001b[1;32m    985\u001b[0m \u001b[39m# if thread has a suspend flag, we suspend with a busy wait\u001b[39;00m\n\u001b[1;32m    986\u001b[0m \u001b[39mif\u001b[39;00m info\u001b[39m.\u001b[39mpydev_state \u001b[39m==\u001b[39m STATE_SUSPEND:\n\u001b[0;32m--> 987\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdo_wait_suspend(thread, frame, event, arg)\n\u001b[1;32m    988\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrace_dispatch\n\u001b[1;32m    989\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.15/envs/fistik/lib/python3.9/site-packages/debugpy/_vendored/pydevd/_pydevd_bundle/pydevd_frame.py:164\u001b[0m, in \u001b[0;36mPyDBFrame.do_wait_suspend\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdo_wait_suspend\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m--> 164\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_args[\u001b[39m0\u001b[39;49m]\u001b[39m.\u001b[39;49mdo_wait_suspend(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.15/envs/fistik/lib/python3.9/site-packages/debugpy/_vendored/pydevd/pydevd.py:2062\u001b[0m, in \u001b[0;36mPyDB.do_wait_suspend\u001b[0;34m(self, thread, frame, event, arg, exception_type)\u001b[0m\n\u001b[1;32m   2059\u001b[0m             from_this_thread\u001b[39m.\u001b[39mappend(frame_custom_thread_id)\n\u001b[1;32m   2061\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_threads_suspended_single_notification\u001b[39m.\u001b[39mnotify_thread_suspended(thread_id, stop_reason):\n\u001b[0;32m-> 2062\u001b[0m         keep_suspended \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_do_wait_suspend(thread, frame, event, arg, suspend_type, from_this_thread, frames_tracker)\n\u001b[1;32m   2064\u001b[0m frames_list \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   2066\u001b[0m \u001b[39mif\u001b[39;00m keep_suspended:\n\u001b[1;32m   2067\u001b[0m     \u001b[39m# This means that we should pause again after a set next statement.\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.15/envs/fistik/lib/python3.9/site-packages/debugpy/_vendored/pydevd/pydevd.py:2098\u001b[0m, in \u001b[0;36mPyDB._do_wait_suspend\u001b[0;34m(self, thread, frame, event, arg, suspend_type, from_this_thread, frames_tracker)\u001b[0m\n\u001b[1;32m   2095\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_input_hook()\n\u001b[1;32m   2097\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprocess_internal_commands()\n\u001b[0;32m-> 2098\u001b[0m     time\u001b[39m.\u001b[39;49msleep(\u001b[39m0.01\u001b[39;49m)\n\u001b[1;32m   2100\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcancel_async_evaluation(get_current_thread_id(thread), \u001b[39mstr\u001b[39m(\u001b[39mid\u001b[39m(frame)))\n\u001b[1;32m   2102\u001b[0m \u001b[39m# process any stepping instructions\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "dict_pipelines = {}\n",
    "y_test_preds  = [] \n",
    "\n",
    "y_val_preds = defaultdict(list)\n",
    "errors = defaultdict(list)\n",
    "\n",
    "\n",
    "for model_i in range(3):\n",
    "\n",
    "    train_y_i = train_y.iloc[:, model_i]\n",
    "    \n",
    "    cv_args = {\"test_size\": 1, \"n_splits\": n_SPLITS, \"train_size\": n_TRAIN_TRAIN_SIZE, 'gap_size': 0}\n",
    "    \n",
    "    cv = GroupTimeSeriesSplit(**cv_args)\n",
    "\n",
    "    # NUMERICAL FEATURES\n",
    "    target_features = Pipeline([('select', tools.ColumnSelector(features=list_target_features[model_i]))])\n",
    "    hierarchy_features = Pipeline([('select', tools.ColumnSelector(features=list_hierarchy_features[model_i]))])\n",
    "    leak_features = Pipeline([('select', tools.ColumnSelector(features=TARGETS[model_i]))])\n",
    "    \n",
    "    categorical_features = Pipeline(\n",
    "        [\n",
    "        ('categorical_features', CategoricalFeatureEngineering(TARGETS[model_i])),\n",
    "        ])\n",
    "\n",
    "\n",
    "    merge_features = FeatureUnion([\n",
    "        \n",
    "        ('target_features', target_features),\n",
    "        ('hierarchy_features', hierarchy_features),\n",
    "        # ('leak_features', leak_features),\n",
    "        ('categorical_features', categorical_features),\n",
    "\n",
    "    \n",
    "\n",
    "    ])\n",
    "\n",
    "    general_features = Pipeline(\n",
    "        [\n",
    "        ('remove_outliers', Winsorizer()),\n",
    "        ('standart_scalar', StandardScaler())\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    model = TransformedTargetRegressor(regressor=LinearRegression())\n",
    "    model = TransformedTargetRegressor(regressor=lgb.LGBMRegressor())\n",
    "    # model = TransformedTargetRegressor(regressor=RandomForestRegressor())\n",
    "\n",
    "    model_pipeline = Pipeline([\n",
    "        (\"feature_engineering\", merge_features),\n",
    "        # (\"general_features\", general_features),\n",
    "        (\"model\", model)\n",
    "    ])\n",
    "\n",
    "    \n",
    "    param_dists = {\n",
    "    \"model__regressor__num_leaves\": [5,10,30,50],\n",
    "    \"model__regressor__max_depth\": [3, 5,10,30,50],\n",
    "    \"model__regressor__learning_rate\": [0.1, 0.05, 0.01],\n",
    "    \"model__regressor__n_estimators\": [300, 500,1000,3000,5000],\n",
    "    # \"model__regressor__min_child_samples\": [10,20,30],\n",
    "    \"model__regressor__subsample\": [.8,.9,1],\n",
    "    \"model__regressor__colsample_bytree\": [.8,.9,1],\n",
    "    \"model__regressor__reg_alpha\": [.05,.1,.3,.5],\n",
    "    \"model__regressor__reg_lambda\": [.1,.3,.5],\n",
    "    # \"model__regressor__random_state\": [1881],\n",
    "    \"model__regressor__objective\": ['mean_squared_error','mean_absolute_error'],    \n",
    "    }\n",
    "    \n",
    "    # param_dists = {\n",
    "    #     \"model__regressor__C\": [.1,.4,.6,.9,1],\n",
    "        \n",
    "    #     }\n",
    "    # param_dists = {}\n",
    "\n",
    "\n",
    "    grid = RandomizedSearchCV(\n",
    "                model_pipeline,\n",
    "                param_distributions=param_dists,\n",
    "                scoring=make_scorer(tools.SMAPE_1, greater_is_better=False), \n",
    "                cv=cv,\n",
    "                # n_jobs=-1,\n",
    "                n_iter=50)\n",
    "    \n",
    "    grid.fit(train_X, train_y_i, groups=train_X['dcount'])\n",
    "\n",
    "    dict_pipelines[f'pipeline_model_{model_i}'] = grid\n",
    "    \n",
    "    # print(grid.cv_results_)\n",
    "    # print(grid.best_estimator_)\n",
    "    \n",
    "    # ## CHECK\n",
    "\n",
    "    check_train_period = TRAIN_PERIOD[-1-n_TRAIN_TRAIN_SIZE: -1] \n",
    "    check_validation_period = [TRAIN_PERIOD[-1]]\n",
    "\n",
    "    best_model = grid.best_estimator_\n",
    "    check_train_index = train_X.query('dcount in @check_train_period').index\n",
    "    best_model.fit(train_X.loc[check_train_index], train_y_i.loc[check_train_index])  \n",
    "    \n",
    "    val_index = train_X.query('dcount in @check_validation_period').index\n",
    "    # y_val_pred =  best_model.predict(train_X.loc[val_index])\n",
    "    print(f'active_lag_{model_i+2+1}')\n",
    "    y_val_pred =  (best_model.predict(train_X.loc[val_index])+1)*train_X.loc[val_index,f'active_lag_{model_i+2+1}']\t\n",
    "    y_val_preds[f'pred_target_{model_i}'] = y_val_pred\n",
    "    errors[f'error_{model_i}'] = tools.SMAPE_1(y_true= train_X.loc[val_index,'active'], y_pred=y_val_pred)\n",
    "\n",
    "    # INFERENCE\n",
    "    # final_train_period = TRAIN_PERIOD[-n_TRAIN_TRAIN_SIZE:] \n",
    "    # final_train_index = train_X.query('dcount in @final_train_period').index\n",
    "    # TEST_PERIOD_i = [TEST_PERIOD[model_i]]\n",
    "\n",
    "    # best_model.fit(train_X.loc[final_train_index], train_y_i.loc[final_train_index])\n",
    "    # y_test_preds.append(best_model.predict(test_X.query('dcount in @TEST_PERIOD_i')))\n",
    "    break\n",
    "# # prepare test the output\n",
    "# test_X['ratio_pred'] = np.concatenate((y_test_preds))\n",
    "\n",
    "# # prepare validation for error analysis\n",
    "val_X = train_X.query('dcount in @check_validation_period')\n",
    "y_val_preds =  pd.DataFrame(y_val_preds, index=val_X.index)\n",
    "val_X = pd.concat((val_X, y_val_preds), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model__regressor__objective': 'mean_absolute_error',\n",
       " 'model__regressor__n_estimators': 3000,\n",
       " 'model__regressor__learning_rate': 0.05}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "defaultdict(list, {'error_0': 2.965482969484996})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(dict_pipelines['pipeline_model_0'].best_params_)\n",
    "display(errors)\n",
    "# condition = val_X['population']<np.quantile(val_X['population'],q=.3)\n",
    "# (tools.SMAPE_1(val_X.loc[condition]['active'],val_X.loc[condition]['pred_target_0']),tools.SMAPE_1(val_X.loc[~condition]['active'],val_X.loc[~condition]['pred_target_0']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defaultdict(list, {'error_0': 2.8409474379596613})\n",
    "# defaultdict(list,\n",
    "#             {'error_0': 2.8770759725599087,\n",
    "#              'error_1': 3.395943795656617,\n",
    "#              'error_2': 4.835238471998951})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_X['dif'] = np.abs(val_X['pred_target_0']-val_X['active'])\n",
    "val_X.sort_values(['dif'],ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample.query('cfips == \"56033\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(0.344+1)*238245.000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_X = test_X.sort_values(['cfips','first_day_of_month'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_X['benchmark'] = test_X.groupby('cfips').first()[['active_lag_1','active_lag_1','active_lag_1']].stack().values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,TEST_PERIOD_i in enumerate(TEST_PERIOD):\n",
    "    test_index = test_X.query('dcount == @TEST_PERIOD_i').index \n",
    "    test_X.loc[test_index,'pred'] = (test_X.loc[test_index]['ratio_pred']+1)*test_X.loc[test_index][f'active_lag_{i+1}']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_id = np.random.choice(train_X['cfips'])\n",
    "\n",
    "plt.plot(train_X.query('cfips==@random_id')['dcount'],train_X.query('cfips==@random_id')['microbusiness_density'])\n",
    "plt.plot(test_X.query('cfips==@random_id')['dcount'],test_X.query('cfips==@random_id')['microbusiness_density'])\n",
    "# plt.plot(test_X.query('cfips==@random_id')['dcount'],test_X.query('cfips==@random_id')['benchmark'], '--')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_X['final_pred'] = test_X['pred']\n",
    "condition = test_X['population']<np.quantile(test_X['population'],q=.3)\n",
    "test_X.loc[condition,'final_pred'] = test_X.loc[condition,'benchmark']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # Prepare submission file\n",
    "\n",
    "# microbusiness_density = 100 * active / adult_population\n",
    "# 100*data.head()['active'] / data.head()['population']\n",
    "# test_X['microbusiness_density'] = 100 * test_X['active'] / test_X['population']\n",
    "\n",
    "date_submission = '0903'\n",
    "local_score = round(errors['error_0'],2)\n",
    "model_name = 'ratio_regression_lag_1_2_with_.4_constant'\n",
    "\n",
    "df_output = test_X.assign(\n",
    "    microbusiness_density = lambda df: 100 * df['final_pred'] / df['population'],\n",
    "    row_id = lambda df: df.apply(lambda df: \"{}_{}\".format(int(df['cfips']),df['date']), axis='columns'))[['row_id','microbusiness_density']]\n",
    "\n",
    "submission = pd.concat((\n",
    "    df_output,\n",
    "    sample_submission[~sample_submission.row_id.isin(df_output.row_id)]))\n",
    "\n",
    "submission.to_csv(f\"data/{date_submission}_{model_name}_local_{local_score}.csv\",index=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ERROR ANALYSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# val_X['target_benchmark'] = val_X['active_lag1']\n",
    "# val_X['error_0']= val_X[['active_t0','target_0']].apply(lambda x: tools.SMAPE_1(x[[0]],x[[1]]),axis=1)\n",
    "# val_X['error_benchmark']= val_X[['active_t0','target_benchmark']].apply(lambda x: tools.SMAPE_1(x[[0]],x[[1]]),axis=1)\n",
    "# val_X['error_1']= val_X[['active_t0','target_1']].apply(lambda x: tools.SMAPE_1(x[[0]],x[[1]]),axis=1)\n",
    "# val_X['error_2']= val_X[['active_t0','target_2']].apply(lambda x: tools.SMAPE_1(x[[0]],x[[1]]),axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_errors = val_X.sort_values('error_0', ascending=False)\n",
    "# df_errors['cum_error'] = df_errors['error_0'].expanding().mean()\n",
    "# df_errors['cum_error_benchmark'] = df_errors['error_benchmark'].expanding().mean()\n",
    "# # val_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_errors['cum_error'].plot()\n",
    "# df_errors['cum_error_benchmark'].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# errors[errors['error_0']>20]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# errors['c_population']=  pd.cut(np.log1p(errors['population']),5)\n",
    "# errors['c_population'].value_counts()\n",
    "# errors.groupby(['c_population'])['error_0'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# errors = errors[errors['error_0']>1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.scatter( np.log1p(errors['population']), np.log1p(errors['error_0']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ad1d413a8513d632047e434ee4038ec414e54bcf3dba008e5fb6dedaf37ce15f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
