{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from feature_engine.outliers import Winsorizer\n",
    "from sklearn.dummy import DummyRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler,MinMaxScaler\n",
    "from feature_engine.outliers import Winsorizer\n",
    "from sklearn.compose import TransformedTargetRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn import set_config, get_config\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer\n",
    "from pprint import pprint\n",
    "from collections import defaultdict\n",
    "\n",
    "set_config(transform_output=\"pandas\")\n",
    "from mlxtend.evaluate.time_series import GroupTimeSeriesSplit, plot_splits, print_cv_info, print_split_info\n",
    "\n",
    "\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.options.display.float_format = '{:.3f}'.format\n",
    "import warnings; warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SMAPE_1 (y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Symmetric Mean Absolute Percentage Error (SMAPE)\n",
    "    \"\"\"\n",
    "    y_true = np.array(y_true)\n",
    "    denominator = (np.abs(y_true) + np.abs(y_pred)) / 200.0\n",
    "    diff = np.abs(y_true - y_pred) / denominator\n",
    "    diff[denominator == 0] = 0.0\n",
    "    return np.mean(diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_train = pd.read_csv('./data/raw/godaddy-microbusiness-density-forecasting/train.csv')\n",
    "new_train = pd.read_csv('./data/raw/godaddy-microbusiness-density-forecasting_new/revealed_test.csv')\n",
    "\n",
    "old_test = pd.read_csv('./data/raw/godaddy-microbusiness-density-forecasting/test.csv')\n",
    "sample_submission = pd.read_csv('./data/raw/godaddy-microbusiness-density-forecasting/sample_submission.csv')\n",
    "\n",
    "train = pd.concat((old_train, new_train))\n",
    "test = old_test[~old_test['first_day_of_month'].isin(new_train['first_day_of_month'])]\n",
    "\n",
    "train['is_test'] = 0 ; test['is_test'] = 1\n",
    "\n",
    "data = pd.concat((\n",
    "        train,\n",
    "        test)\n",
    "        )\\\n",
    "    .reset_index(drop=True)\\\n",
    "    .assign(\n",
    "        cfips = lambda df: df['cfips'].astype(str).str.zfill(5),\n",
    "        date = lambda df: pd.to_datetime(df[\"first_day_of_month\"]).dt.date,\n",
    "        mdensity_t0 = lambda df: df['microbusiness_density'],\n",
    "        )\\\n",
    "    .sort_values(['cfips','date'], ascending=True)\\\n",
    "    .assign(\n",
    "    \n",
    "        state_i = lambda df: df['cfips'].apply(lambda x: x[:2]),\n",
    "        county_i = lambda df: df['cfips'].apply(lambda x: x[2:]),\n",
    "        \n",
    "        # year = lambda df: df['date'].dt.year,\n",
    "        # month = lambda df: df['date'].dt.month,\n",
    "\n",
    "        dcount = lambda df: df.groupby('cfips')['row_id'].cumcount(),\n",
    "        \n",
    "        mdensity_lag1 = lambda df: df.groupby('cfips')['mdensity_t0'].shift(1),\n",
    "        mdensity_lag2 = lambda df: df.groupby('cfips')['mdensity_t0'].shift(2),\n",
    "        mdensity_lag3 = lambda df: df.groupby('cfips')['mdensity_t0'].shift(3),\n",
    "        \n",
    "        target_0 = lambda df: np.nan_to_num(df['mdensity_t0']),\n",
    "        target_1 = lambda df: np.nan_to_num(df['mdensity_t0']),\n",
    "        target_2 = lambda df: np.nan_to_num(df['mdensity_t0']),\n",
    "\n",
    "    \n",
    "\n",
    "    )\\\n",
    "    .drop(['county','state'], axis='columns')\n",
    "# .sort_index(ascending=True)\n",
    "\n",
    "assert all(data.groupby('cfips')['county_i'].nunique() == 1)\n",
    "assert all(data.groupby('cfips')['state_i'].nunique() == 1)\n",
    "assert data['cfips'].nunique() == 3135 # there are 3135 county,state tuples\n",
    "assert data['dcount'].nunique() == 47 # there are 47 series for each county state tuple\n",
    "assert data.query('is_test==0')['dcount'].nunique() == 41 # there are 41 series in the train set. \n",
    "assert data.query('is_test==1')['dcount'].nunique() == 6  # there are 6 series in the test set. \n",
    "\n",
    "#The private leaderboard will include 03-2023, 04-2023, 05-2023\n",
    "#The public leaderboard includes the first month 11-2022. Probably it will be updated later as 12-2022,01-2023 and 02-2023\n",
    "#The LB is updated as 01-2023\n",
    "\n",
    "# capper = Winsorizer(capping_method='iqr',tail='both', fold=5)\n",
    "# data['target_0'] = capper.fit_transform(data[['target_0']])\n",
    "# data['target_1'] = capper.fit_transform(data[['target_1']])\n",
    "# data['target_2'] = capper.fit_transform(data[['target_2']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_id</th>\n",
       "      <th>cfips</th>\n",
       "      <th>first_day_of_month</th>\n",
       "      <th>microbusiness_density</th>\n",
       "      <th>active</th>\n",
       "      <th>is_test</th>\n",
       "      <th>date</th>\n",
       "      <th>mdensity_t0</th>\n",
       "      <th>state_i</th>\n",
       "      <th>county_i</th>\n",
       "      <th>dcount</th>\n",
       "      <th>mdensity_lag1</th>\n",
       "      <th>mdensity_lag2</th>\n",
       "      <th>mdensity_lag3</th>\n",
       "      <th>target_0</th>\n",
       "      <th>target_1</th>\n",
       "      <th>target_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>131670</th>\n",
       "      <td>1001_2023-02-01</td>\n",
       "      <td>01001</td>\n",
       "      <td>2023-02-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>2023-02-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>01</td>\n",
       "      <td>001</td>\n",
       "      <td>42</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.471</td>\n",
       "      <td>3.443</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134805</th>\n",
       "      <td>1001_2023-03-01</td>\n",
       "      <td>01001</td>\n",
       "      <td>2023-03-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>2023-03-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>01</td>\n",
       "      <td>001</td>\n",
       "      <td>43</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.471</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137940</th>\n",
       "      <td>1001_2023-04-01</td>\n",
       "      <td>01001</td>\n",
       "      <td>2023-04-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>2023-04-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>01</td>\n",
       "      <td>001</td>\n",
       "      <td>44</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141075</th>\n",
       "      <td>1001_2023-05-01</td>\n",
       "      <td>01001</td>\n",
       "      <td>2023-05-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>2023-05-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>01</td>\n",
       "      <td>001</td>\n",
       "      <td>45</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144210</th>\n",
       "      <td>1001_2023-06-01</td>\n",
       "      <td>01001</td>\n",
       "      <td>2023-06-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>2023-06-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>01</td>\n",
       "      <td>001</td>\n",
       "      <td>46</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 row_id  cfips first_day_of_month  microbusiness_density  active  is_test        date  mdensity_t0 state_i county_i  dcount  mdensity_lag1  mdensity_lag2  mdensity_lag3  target_0  target_1  target_2\n",
       "131670  1001_2023-02-01  01001         2023-02-01                    NaN     NaN        1  2023-02-01          NaN      01      001      42            NaN          3.471          3.443     0.000     0.000     0.000\n",
       "134805  1001_2023-03-01  01001         2023-03-01                    NaN     NaN        1  2023-03-01          NaN      01      001      43            NaN            NaN          3.471     0.000     0.000     0.000\n",
       "137940  1001_2023-04-01  01001         2023-04-01                    NaN     NaN        1  2023-04-01          NaN      01      001      44            NaN            NaN            NaN     0.000     0.000     0.000\n",
       "141075  1001_2023-05-01  01001         2023-05-01                    NaN     NaN        1  2023-05-01          NaN      01      001      45            NaN            NaN            NaN     0.000     0.000     0.000\n",
       "144210  1001_2023-06-01  01001         2023-06-01                    NaN     NaN        1  2023-06-01          NaN      01      001      46            NaN            NaN            NaN     0.000     0.000     0.000"
      ]
     },
     "execution_count": 286,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[data['cfips'] == '01001'].tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data[data['cfips'] == '01001'].tail(20)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PARAMETERS\n",
    "n_SPLITS = 5 \n",
    "n_TRAIN_TRAIN_SIZE = 3\n",
    "n_TRAIN_PERIOD = n_TRAIN_TRAIN_SIZE + 3 + n_SPLITS - 1 \n",
    "\n",
    "\n",
    "TEST_DATES = list(np.sort(data.query('is_test==1')['date'].unique())[:3])\n",
    "TEST_PERIOD = list(np.sort(data.query('is_test==1')['dcount'].unique())[:3])\n",
    "\n",
    "TRAIN_PERIOD = list(np.sort(data.query('is_test==0')['dcount'].unique())[-n_TRAIN_PERIOD:])\n",
    "TRAIN_DATES = list(np.sort(data.query('is_test==0')['date'].unique())[-n_TRAIN_PERIOD:])\n",
    "\n",
    "LEAKAGE = ['mdensity_t0']\n",
    "TARGETS = ['target_0', 'target_1', 'target_2']\n",
    "LAG_DENSITY = ['mdensity_lag1', 'mdensity_lag2', 'mdensity_lag3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data[data['dcount'].isin(TEST_PERIOD)].head()\n",
    "# sample = data[data.cfips.isin(['01001'])] # sample = data[data.cfips.isin(['01001','56045'])]\n",
    "sample = data.copy()\n",
    "sample.loc[sample.is_test==1,TARGETS]  = np.nan\n",
    "sample = sample.set_index(['date','cfips']).sort_index().loc[TRAIN_DATES+TEST_DATES]\n",
    "sample = sample[['dcount','county_i'] + LAG_DENSITY + TARGETS + LEAKAGE]\n",
    "sample_train= sample.query(\"dcount in @TRAIN_PERIOD\") ; sample_test= sample.query(\"dcount in @TEST_PERIOD\")\n",
    "train_X = sample_train.drop(TARGETS,axis='columns') ; train_y = sample_train[TARGETS]\n",
    "test_X = sample_test.drop(TARGETS,axis='columns') ; test_y = sample_test[TARGETS]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipelining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tools import ColumnSelector, LagModel, create_submission\n",
    "from sklearn.ensemble import GradientBoostingRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic_pipelines = {}\n",
    "y_test_preds  = [] \n",
    "\n",
    "y_val_preds = defaultdict(list)\n",
    "errors = defaultdict(list)\n",
    "\n",
    "lag=1\n",
    "list_cols_model = [[f'mdensity_lag{lag_i+model_i+1}' for lag_i in range(lag)] for model_i in range(3)]\n",
    "\n",
    "for model_i in range(3):\n",
    "\n",
    "    train_y_i = train_y.iloc[:, model_i]\n",
    "    \n",
    "    cv_args = {\"test_size\": 1, \"n_splits\": n_SPLITS, \"train_size\": n_TRAIN_TRAIN_SIZE, 'gap_size': 0}\n",
    "    cv = GroupTimeSeriesSplit(**cv_args)\n",
    "\n",
    "    # new_features = Pipeline([('select', SimpleFeatureEngineering(features=list_cols_model[model_i]))])\n",
    "    # print(list_cols_model[model_i])\n",
    "    raw_features = Pipeline([('select', ColumnSelector(features=list_cols_model[model_i]))])\n",
    "    \n",
    "    merge_features_numeric = FeatureUnion([\n",
    "        # ('new_features', new_features),\n",
    "        ('raw_features', raw_features)\n",
    "    ])\n",
    "\n",
    "    final_features_numeric = Pipeline([\n",
    "                            ('merge_features',merge_features_numeric),\n",
    "                            # ('remove_outliers', Winsorizer(capping_method='iqr', tail='both',fold=3)),\n",
    "                            # ('standart_scaler', StandardScaler())\n",
    "                            ]\n",
    "                            )\n",
    "\n",
    "    \n",
    "    # model = TransformedTargetRegressor(regressor=DummyRegressor(strategy='median'), transformer=None)\n",
    "    model = TransformedTargetRegressor(regressor=LagModel(), transformer=None)\n",
    "    \n",
    "    model_pipeline = Pipeline([\n",
    "        (\"transform\", final_features_numeric),\n",
    "        (\"model\", model)\n",
    "    ])\n",
    "\n",
    "    dic_pipelines[f'pipeline_model_{model_i}'] = model_pipeline\n",
    "    \n",
    "    # param_grid = {'model__regressor__strategy':['mean','median' ]}\n",
    "    param_grid = {}\n",
    "    grid = GridSearchCV(dic_pipelines[f'pipeline_model_{model_i}'], scoring=make_scorer(SMAPE_1, greater_is_better=False), param_grid=param_grid, cv=cv)\n",
    "    grid.fit(train_X, train_y_i, groups=train_X['dcount'])\n",
    "\n",
    "    # print(grid.cv_results_)\n",
    "    \n",
    "    # print(grid.best_estimator_)\n",
    "    \n",
    "    ## CHECK\n",
    "    check_train_period = TRAIN_DATES[-1-n_TRAIN_TRAIN_SIZE: -1] \n",
    "    validation_period = TRAIN_DATES[-1] \n",
    "\n",
    "    best_model = grid.best_estimator_\n",
    "\n",
    "    best_model.fit(train_X.loc[check_train_period], train_y_i.loc[check_train_period])   \n",
    "    \n",
    "    y_val_pred =  best_model.predict(train_X.loc[validation_period])   \n",
    "    y_val_preds[f'target_{model_i}'] = y_val_pred\n",
    "    y_val_i = train_y_i.loc[validation_period]    \n",
    "    errors[f'error_{model_i}'] = SMAPE_1(y_true=y_val_i, y_pred=y_val_pred)\n",
    "\n",
    "#     # INFERENCE\n",
    "    final_train_period = TRAIN_DATES[-n_TRAIN_TRAIN_SIZE:] \n",
    "\n",
    "    best_model.fit(train_X.loc[final_train_period], train_y_i.loc[final_train_period])   \n",
    "\n",
    "    y_test_pred =  best_model.predict(test_X.loc[TEST_DATES[model_i]] )\n",
    "    y_test_preds.append(y_test_pred)\n",
    "\n",
    "test_X['microbusiness_density'] = np.concatenate((y_test_preds))\n",
    "\n",
    "# prepare data for error analysis\n",
    "val_X = train_X.loc[validation_period]\n",
    "y_val_preds =  pd.DataFrame(y_val_preds, index=val_X.index)\n",
    "val_X = pd.concat((val_X, y_val_preds), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(list,\n",
       "            {'error_0': 1.889206717018118,\n",
       "             'error_1': 2.4787836068174856,\n",
       "             'error_2': 2.9725080424835664})"
      ]
     },
     "execution_count": 292,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'011'"
      ]
     },
     "execution_count": 293,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = 11\n",
    "f\"{x:03}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "COLS = ['GEO_ID','NAME','S0101_C01_026E']\n",
    "df2020 = pd.read_csv('./data/raw/census_data_1/ACSST5Y2020.S0101-Data.csv',usecols=COLS)\n",
    "df2020 = df2020.iloc[1:]\n",
    "df2020['S0101_C01_026E'] = df2020['S0101_C01_026E'].astype('int')\n",
    "\n",
    "COLS = ['GEO_ID','NAME','S0101_C01_026E']\n",
    "df2021 = pd.read_csv('./data/raw/census_data_1/ACSST5Y2021.S0101-Data.csv',usecols=COLS)\n",
    "df2021 = df2021.iloc[1:]\n",
    "df2021['S0101_C01_026E'] = df2021['S0101_C01_026E'].astype('int')\n",
    "\n",
    "df2020['cfips'] = df2020.GEO_ID.apply(lambda x: f\"{int(x.split('US')[-1]):05}\" )\n",
    "adult2020 = df2020.set_index('cfips').S0101_C01_026E.to_dict()\n",
    "\n",
    "df2021['cfips'] = df2021.GEO_ID.apply(lambda x: f\"{int(x.split('US')[-1]):05}\" )\n",
    "adult2021 = df2021.set_index('cfips').S0101_C01_026E.to_dict()\n",
    "\n",
    "sub = test_X.reset_index()\n",
    "sub['adult2020'] = sub.cfips.map(adult2020)\n",
    "sub['adult2021'] = sub.cfips.map(adult2021)\n",
    "\n",
    "sub.microbusiness_density = sub.microbusiness_density * sub.adult2020 / sub.adult2021\n",
    "# sub = sub.drop(['adult2020','adult2021','cfips'],axis=1)\n",
    "# sub.to_csv('submission.csv',index=False)\n",
    "# sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_test_X = sub.set_index(['date','cfips'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "submission is created for date: 0103 model: adjusted_lag_1 with score: 1.89\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_id</th>\n",
       "      <th>microbusiness_density</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1001_2023-01-01</td>\n",
       "      <td>3.319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1003_2023-01-01</td>\n",
       "      <td>7.935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1005_2023-01-01</td>\n",
       "      <td>1.240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1007_2023-01-01</td>\n",
       "      <td>1.287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1009_2023-01-01</td>\n",
       "      <td>1.819</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            row_id  microbusiness_density\n",
       "0  1001_2023-01-01                  3.319\n",
       "1  1003_2023-01-01                  7.935\n",
       "2  1005_2023-01-01                  1.240\n",
       "3  1007_2023-01-01                  1.287\n",
       "4  1009_2023-01-01                  1.819"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# # Prepare submission file\n",
    "\n",
    "date_submission = '0103'\n",
    "local_score = round(errors['error_0'],2)\n",
    "model_name = 'adjusted_lag_1'\n",
    "\n",
    "submission = create_submission(new_test_X,date_submission, model_name, local_score, sample_submission)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_id</th>\n",
       "      <th>cfips</th>\n",
       "      <th>first_day_of_month</th>\n",
       "      <th>microbusiness_density</th>\n",
       "      <th>active</th>\n",
       "      <th>is_test</th>\n",
       "      <th>date</th>\n",
       "      <th>mdensity_t0</th>\n",
       "      <th>state_i</th>\n",
       "      <th>county_i</th>\n",
       "      <th>dcount</th>\n",
       "      <th>mdensity_lag1</th>\n",
       "      <th>mdensity_lag2</th>\n",
       "      <th>mdensity_lag3</th>\n",
       "      <th>target_0</th>\n",
       "      <th>target_1</th>\n",
       "      <th>target_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>1001_2022-06-01</td>\n",
       "      <td>01001</td>\n",
       "      <td>2022-06-01</td>\n",
       "      <td>3.346</td>\n",
       "      <td>1422.000</td>\n",
       "      <td>0</td>\n",
       "      <td>2022-06-01</td>\n",
       "      <td>3.346</td>\n",
       "      <td>01</td>\n",
       "      <td>001</td>\n",
       "      <td>34</td>\n",
       "      <td>3.313</td>\n",
       "      <td>3.372</td>\n",
       "      <td>3.337</td>\n",
       "      <td>3.346</td>\n",
       "      <td>3.346</td>\n",
       "      <td>3.346</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             row_id  cfips first_day_of_month  microbusiness_density   active  is_test        date  mdensity_t0 state_i county_i  dcount  mdensity_lag1  mdensity_lag2  mdensity_lag3  target_0  target_1  target_2\n",
       "34  1001_2022-06-01  01001         2022-06-01                  3.346 1422.000        0  2022-06-01        3.346      01      001      34          3.313          3.372          3.337     3.346     3.346     3.346"
      ]
     },
     "execution_count": 311,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.query('(cfips == \"01001\") and (first_day_of_month== \"2022-06-01\")')\n",
    "\n",
    "# 42496"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.3461972891566263"
      ]
     },
     "execution_count": 312,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1422.000/42496*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22.00819118772222"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 7.090 = \n",
    "100 * 9780/44438\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ERROR ANALYSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for model_i in range(3):\n",
    "    val_X[f'error_{model_i}'] = val_X.apply(lambda x: SMAPE_1([x['mdensity_t0']],[x[f'target_{model_i}']]),axis='columns')\n",
    "\n",
    "# x = val_X['error_0'].expanding().mean()\n",
    "# x = x[x>4]\n",
    "# x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dcount</th>\n",
       "      <th>county_i</th>\n",
       "      <th>mdensity_lag1</th>\n",
       "      <th>mdensity_lag2</th>\n",
       "      <th>mdensity_lag3</th>\n",
       "      <th>mdensity_t0</th>\n",
       "      <th>target_0</th>\n",
       "      <th>target_1</th>\n",
       "      <th>target_2</th>\n",
       "      <th>error_0</th>\n",
       "      <th>error_1</th>\n",
       "      <th>error_2</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cfips</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>01001</th>\n",
       "      <td>40</td>\n",
       "      <td>001</td>\n",
       "      <td>3.443</td>\n",
       "      <td>3.464</td>\n",
       "      <td>3.443</td>\n",
       "      <td>3.471</td>\n",
       "      <td>3.443</td>\n",
       "      <td>3.464</td>\n",
       "      <td>3.443</td>\n",
       "      <td>0.817</td>\n",
       "      <td>0.204</td>\n",
       "      <td>0.817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>01003</th>\n",
       "      <td>40</td>\n",
       "      <td>003</td>\n",
       "      <td>8.258</td>\n",
       "      <td>8.360</td>\n",
       "      <td>8.342</td>\n",
       "      <td>8.251</td>\n",
       "      <td>8.258</td>\n",
       "      <td>8.360</td>\n",
       "      <td>8.342</td>\n",
       "      <td>0.085</td>\n",
       "      <td>1.314</td>\n",
       "      <td>1.098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>01005</th>\n",
       "      <td>40</td>\n",
       "      <td>005</td>\n",
       "      <td>1.247</td>\n",
       "      <td>1.232</td>\n",
       "      <td>1.207</td>\n",
       "      <td>1.252</td>\n",
       "      <td>1.247</td>\n",
       "      <td>1.232</td>\n",
       "      <td>1.207</td>\n",
       "      <td>0.404</td>\n",
       "      <td>1.626</td>\n",
       "      <td>3.696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>01007</th>\n",
       "      <td>40</td>\n",
       "      <td>007</td>\n",
       "      <td>1.276</td>\n",
       "      <td>1.287</td>\n",
       "      <td>1.315</td>\n",
       "      <td>1.287</td>\n",
       "      <td>1.276</td>\n",
       "      <td>1.287</td>\n",
       "      <td>1.315</td>\n",
       "      <td>0.877</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2.160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>01009</th>\n",
       "      <td>40</td>\n",
       "      <td>009</td>\n",
       "      <td>1.836</td>\n",
       "      <td>1.832</td>\n",
       "      <td>1.852</td>\n",
       "      <td>1.852</td>\n",
       "      <td>1.836</td>\n",
       "      <td>1.832</td>\n",
       "      <td>1.852</td>\n",
       "      <td>0.855</td>\n",
       "      <td>1.101</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56037</th>\n",
       "      <td>40</td>\n",
       "      <td>037</td>\n",
       "      <td>2.814</td>\n",
       "      <td>2.824</td>\n",
       "      <td>2.811</td>\n",
       "      <td>2.877</td>\n",
       "      <td>2.814</td>\n",
       "      <td>2.824</td>\n",
       "      <td>2.811</td>\n",
       "      <td>2.193</td>\n",
       "      <td>1.861</td>\n",
       "      <td>2.304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56039</th>\n",
       "      <td>40</td>\n",
       "      <td>039</td>\n",
       "      <td>26.372</td>\n",
       "      <td>26.273</td>\n",
       "      <td>26.091</td>\n",
       "      <td>26.315</td>\n",
       "      <td>26.372</td>\n",
       "      <td>26.273</td>\n",
       "      <td>26.091</td>\n",
       "      <td>0.218</td>\n",
       "      <td>0.159</td>\n",
       "      <td>0.856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56041</th>\n",
       "      <td>40</td>\n",
       "      <td>041</td>\n",
       "      <td>4.016</td>\n",
       "      <td>4.009</td>\n",
       "      <td>3.996</td>\n",
       "      <td>4.051</td>\n",
       "      <td>4.016</td>\n",
       "      <td>4.009</td>\n",
       "      <td>3.996</td>\n",
       "      <td>0.854</td>\n",
       "      <td>1.026</td>\n",
       "      <td>1.370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56043</th>\n",
       "      <td>40</td>\n",
       "      <td>043</td>\n",
       "      <td>3.143</td>\n",
       "      <td>3.127</td>\n",
       "      <td>3.209</td>\n",
       "      <td>3.060</td>\n",
       "      <td>3.143</td>\n",
       "      <td>3.127</td>\n",
       "      <td>3.209</td>\n",
       "      <td>2.667</td>\n",
       "      <td>2.139</td>\n",
       "      <td>4.749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56045</th>\n",
       "      <td>40</td>\n",
       "      <td>045</td>\n",
       "      <td>1.785</td>\n",
       "      <td>1.785</td>\n",
       "      <td>1.785</td>\n",
       "      <td>1.803</td>\n",
       "      <td>1.785</td>\n",
       "      <td>1.785</td>\n",
       "      <td>1.785</td>\n",
       "      <td>0.995</td>\n",
       "      <td>0.995</td>\n",
       "      <td>0.995</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3135 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       dcount county_i  mdensity_lag1  mdensity_lag2  mdensity_lag3  mdensity_t0  target_0  target_1  target_2  error_0  error_1  error_2\n",
       "cfips                                                                                                                                    \n",
       "01001      40      001          3.443          3.464          3.443        3.471     3.443     3.464     3.443    0.817    0.204    0.817\n",
       "01003      40      003          8.258          8.360          8.342        8.251     8.258     8.360     8.342    0.085    1.314    1.098\n",
       "01005      40      005          1.247          1.232          1.207        1.252     1.247     1.232     1.207    0.404    1.626    3.696\n",
       "01007      40      007          1.276          1.287          1.315        1.287     1.276     1.287     1.315    0.877    0.000    2.160\n",
       "01009      40      009          1.836          1.832          1.852        1.852     1.836     1.832     1.852    0.855    1.101    0.000\n",
       "...       ...      ...            ...            ...            ...          ...       ...       ...       ...      ...      ...      ...\n",
       "56037      40      037          2.814          2.824          2.811        2.877     2.814     2.824     2.811    2.193    1.861    2.304\n",
       "56039      40      039         26.372         26.273         26.091       26.315    26.372    26.273    26.091    0.218    0.159    0.856\n",
       "56041      40      041          4.016          4.009          3.996        4.051     4.016     4.009     3.996    0.854    1.026    1.370\n",
       "56043      40      043          3.143          3.127          3.209        3.060     3.143     3.127     3.209    2.667    2.139    4.749\n",
       "56045      40      045          1.785          1.785          1.785        1.803     1.785     1.785     1.785    0.995    0.995    0.995\n",
       "\n",
       "[3135 rows x 12 columns]"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "# val_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for e in x.index:\n",
    "    plt.figure()\n",
    "    # val_X[val_X['cfips'] == e]['target_0'].plot()\n",
    "    fig = plt.figure()\n",
    "    data[data['cfips'] == e].reset_index(drop=True)['mdensity_t0'].plot()\n",
    "    plt.scatter(38, val_X.loc[e]['target_0'], color='r')\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_sample['error_1']= x_sample[['mdensity_t0','pred']].apply(lambda x: SMAPE_1(x[[0]],x[[1]]),axis=1)\n",
    "# x_sample['error_2']= x_sample[['mdensity_t0','mdensity_lag1']].apply(lambda x: SMAPE_1(x[[0]],x[[1]]),axis=1)a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_sample = x_sample.sort_values(['error_1'],ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# errors = [SMAPE_1(y_val_i[[i]],y_pred[[i]]) for i,y_pred_i in enumerate(y_pred)]\n",
    "# errors = [SMAPE_1(y_val_i[[i]],train_X.loc[TRAIN_DATE[-1:],'mdensity_lag1'].iloc[[i]]) for i,y_pred_i in enumerate(y_pred)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred[np.argsort(errors)]\n",
    "# errors= np.sort(errors)\n",
    "\n",
    "# cum_errors = pd.Series(errors).expanding().mean()\n",
    "# cum_errors.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sample = train_X.loc[TRAIN_DATE[-1:]]\n",
    "train_sample.iloc[np.argsort(errors)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y_sample = train_y.loc[TRAIN_DATE[-1:]]\n",
    "SMAPE_1(train_y_sample.values,np.ones(train_y_sample.shape[0])*np.median(train_y_sample))\n",
    "SMAPE_1(train_y.loc['2022-10-01'].values,train_y.loc['2022-09-01'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 169.0067448718508, 152.1912078949058\n",
    "# 1.0730837144785978, 1.109003414526155\n",
    "# 145.82845110893408, 136.07246720848954\n",
    "# 1.7214972858534574, 1.7328342300564805\n",
    "# 130.85211125280742, 124.7100031585511\n",
    "# 2.348016937375499, 2.3395521962826726"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "black_list = []\n",
    "black_list.extend(train_X.sort_values(['mdensity_t0'])[:50].reset_index()['cfips'].unique())\n",
    "black_list.extend(train_X.sort_values(['mdensity_t0'])[-50:].reset_index()['cfips'].unique())\n",
    "keep = list(set(train_X.reset_index()['cfips'].unique()) - set(black_list))\n",
    "# train_X.loc[(slice(None),keep)]\n",
    "train_X_sample = train_X.loc[(slice(None),keep),:].reset_index().set_index(['date','cfips'])\n",
    "train_y_sample = train_y.loc[(slice(None),keep),:].reset_index().set_index(['date','cfips'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X_sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.linear_model import HuberRegressor, Lasso, Ridge\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "def smape(Y_predict, Y_test):\n",
    "    result = np.linalg.norm(Y_predict - Y_test, axis = 1)\n",
    "    result = np.abs(result)\n",
    "    denom = np.linalg.norm(Y_predict, axis = 1)\n",
    "    denom += np.linalg.norm(Y_test, axis = 1)\n",
    "    result /= denom\n",
    "    result *= 100 * 2\n",
    "    result = np.mean(result)\n",
    "    return result\n",
    "epsilon = 1e-6\n",
    "param_search = np.arange(10, 200, 20)\n",
    "\n",
    "scores = []\n",
    "for i in param_search:\n",
    "    print(i)\n",
    "\n",
    "    # definition of ztransformation.\n",
    "\n",
    "    def ztransform1(Y, param=i):\n",
    "        return 1 / (param + Y)\n",
    "\n",
    "    # inverse transformation, Y = inverseZ(Z)\n",
    "\n",
    "    def inverseZ1(Z, param=i):\n",
    "        return -param + 1 / Z\n",
    "    \n",
    "    \n",
    "    model = TransformedTargetRegressor(GradientBoostingRegressor(loss='squared_error', n_estimators=50,max_depth=10),func= ztransform1, inverse_func=inverseZ1)\n",
    "\n",
    "    model.fit( train_X.loc['2022-05-01':'2022-09-01',['mdensity_lag1','mdensity_lag2','mdensity_lag3']], train_y.loc['2022-05-01':'2022-09-01',['target_0']]) \n",
    "        \n",
    "    print(SMAPE_1(epsilon+model.predict(train_X.loc['2022-02-01':'2022-09-01',['mdensity_lag1','mdensity_lag2','mdensity_lag3']]),train_y.loc['2022-02-01':'2022-09-01',['target_0']].values))\n",
    "    print(SMAPE_1(epsilon+model.predict(train_X.loc['2022-10-01',['mdensity_lag1','mdensity_lag2','mdensity_lag3']]),train_y.loc['2022-10-01',['target_0']].values))\n",
    "    \n",
    "# 160\n",
    "# 1.3528178494715637\n",
    "# 1.4203927419328115\n",
    "# 190\n",
    "# 1.3527584337361627\n",
    "# 1.4161016190620148\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ad1d413a8513d632047e434ee4038ec414e54bcf3dba008e5fb6dedaf37ce15f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
